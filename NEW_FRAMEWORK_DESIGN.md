# ğŸš€ æ–°ä¸€ä»£Titansæ—¶é—´åºåˆ—æ¡†æ¶è®¾è®¡æ–‡æ¡£

## 1. æ ¸å¿ƒç†å¿µ

æœ¬æ¡†æ¶æ—¨åœ¨èåˆ **Titans çš„å…ƒå­¦ä¹ æ€æƒ³** ä¸ **æ—¶é—´åºåˆ—åœ¨çº¿å­¦ä¹ ** çš„å®é™…éœ€æ±‚ï¼Œæ„å»ºä¸€ä¸ªæ¨¡å—åŒ–ã€å¯å®éªŒã€ç¨³å®šä¸”é«˜æ•ˆçš„æŒç»­å­¦ä¹ ç³»ç»Ÿã€‚å®ƒå°†éµå¾ªæˆ‘ä»¬åœ¨ `TITANS_UNDERSTANDING_REVISION.md` ä¸­ä¿®æ­£åçš„ç†è§£ï¼Œç‰¹åˆ«æ˜¯å…³äº**å†…å¤–åŒå¾ªç¯**å’Œ**å…ƒå‚æ•°å­¦ä¹ **çš„æ ¸å¿ƒæœºåˆ¶ã€‚

**æ ¸å¿ƒç›®æ ‡**ï¼š
1.  **å®ç°çœŸæ­£çš„â€œå­¦ä¹ å¦‚ä½•å­¦ä¹ â€**ï¼šæ¡†æ¶çš„æ ¸å¿ƒæ˜¯è®­ç»ƒä¸€ä¸ª**å…ƒå­¦ä¹ å™¨ (Meta-Learner)**ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆé€‚åº”æ•°æ®åŠ¨æ€çš„**LMMæ›´æ–°ç­–ç•¥ï¼ˆå…ƒå‚æ•°ï¼‰**ï¼Œè€Œä¸æ˜¯ç›´æ¥è®­ç»ƒLMMçš„æƒé‡ã€‚
2.  **æ¨¡å—åŒ–è§£è€¦**ï¼šå°†**Backbone (P)**ã€**ç¥ç»è®°å¿†å•å…ƒ (LMM)** å’Œ**å…ƒå­¦ä¹ å™¨ (Meta-Learner)** ä¸‰è€…æ¸…æ™°åˆ†ç¦»ã€‚
3.  **æ”¯æŒæ¸è¿›å¼å®éªŒ**ï¼šæ¡†æ¶å¿…é¡»èƒ½æ”¯æŒä»ç®€å•åˆ°å¤æ‚çš„å¤šç§å®éªŒæ¨¡å¼ï¼ŒåŒ…æ‹¬ï¼š
    *   **æ¨¡å¼1 (Baseline)**ï¼šæ ‡å‡†åœ¨çº¿å­¦ä¹ ï¼ˆæ— LMMï¼‰ã€‚
    *   **æ¨¡å¼2 (Simple TTT)**ï¼šå¸¦LMMï¼Œä½†ä½¿ç”¨å›ºå®šçš„æ›´æ–°ç­–ç•¥ï¼ˆç±»ä¼¼æˆ‘ä»¬ä¹‹å‰çš„å®ç°ï¼‰ã€‚
    *   **æ¨¡å¼3 (Full Meta-TTT)**ï¼šå¸¦LMMï¼Œå¹¶ä½¿ç”¨å…ƒå­¦ä¹ å™¨åŠ¨æ€ç”Ÿæˆæ›´æ–°ç­–ç•¥ï¼ˆæˆ‘ä»¬çš„æœ€ç»ˆç›®æ ‡ï¼‰ã€‚

---

## 2. æ¡†æ¶æ€»ä½“æ¶æ„

```mermaid
graph TD
    subgraph "è¾“å…¥æ•°æ®æµ (Input Stream)"
        direction LR
        X_t[x_t: å½“å‰æ—¶é—´æ­¥æ•°æ®] --> P
    end

    subgraph "æ ¸å¿ƒé¢„æµ‹æ¨¡å— (Core Forecaster)"
        direction LR
        P[Backbone (P)\nç‰¹å¾æå–å™¨] --> F_t[f_t: ç‰¹å¾]
        
        subgraph "ç¥ç»è®°å¿†å•å…ƒ (LMM)"
            M_prev[M_{t-1}\n(LMMæ—§çŠ¶æ€)] -- "è¾“å…¥" --> M_t
            F_t -- "ä½œä¸ºKey/Value" --> M_t
            Meta_t[å…ƒå‚æ•° Î¸_t, Î·_t, Î±_t] -- "æŒ‡å¯¼æ›´æ–°" --> M_t[LMM å†…éƒ¨æ›´æ–°\n(Inner Loop)]
            M_t -- "è¾“å‡º" --> M_curr[M_t\n(LMMæ–°çŠ¶æ€)]
        end

        F_t -- "ä½œä¸ºQuery" --> M_curr
        M_curr -- "æ£€ç´¢è®°å¿†" --> Mem_out[m_t: è®°å¿†è¾“å‡º]

        F_t --> Fusion
        Mem_out --> Fusion[ç‰¹å¾èåˆ]
        Fusion --> PredHead[é¢„æµ‹å¤´]
        PredHead --> Y_pred[y_pred: é¢„æµ‹ç»“æœ]
    end

    subgraph "å…ƒå­¦ä¹ æ¨¡å— (Meta-Learner)"
        direction LR
        F_t -- "æ„ŸçŸ¥ä¸Šä¸‹æ–‡" --> Meta[å…ƒç½‘ç»œ (Meta-Network)]
        Meta --> Meta_t
    end

    subgraph "è®­ç»ƒæµç¨‹ (Training Flow)"
        direction TB
        subgraph "å¤–å¾ªç¯ (Outer Loop)"
            Y_pred --> Loss[æœ€ç»ˆä»»åŠ¡æŸå¤± L_task]
            Y_true[y_true: çœŸå®å€¼] --> Loss
            Loss -- "åå‘ä¼ æ’­" --> Optimizer[AdamW ä¼˜åŒ–å™¨]
            Optimizer -- "æ›´æ–°" --> P
            Optimizer -- "æ›´æ–°" --> Meta
            Optimizer -- "æ›´æ–°" --> PredHead
        end
    end

    style LMM fill:#f9f,stroke:#333,stroke-width:2px
    style Meta-Learner fill:#ccf,stroke:#333,stroke-width:2px
```

### 2.1 æ ¸å¿ƒç»„ä»¶è¯´æ˜

1.  **Backbone (P)** (`models/backbones.py`)
    *   **èŒè´£**ï¼šä»åŸå§‹è¾“å…¥ `x_t` ä¸­æå–é«˜çº§ç‰¹å¾ `f_t`ã€‚
    *   **å®ç°**ï¼šå¯ä»¥æ˜¯ `LSTM`, `Transformer` æˆ– `TitansBackbone`ã€‚
    *   **è®­ç»ƒ**ï¼šå…¶å‚æ•°ç”±**å¤–å¾ªç¯**çš„ `AdamW` ä¼˜åŒ–å™¨æ›´æ–°ã€‚

2.  **ç¥ç»è®°å¿†å•å…ƒ (LMM)** (`models/memory.py`)
    *   **èŒè´£**ï¼šä½œä¸ºä¸€ä¸ª**å¿«é€Ÿæƒé‡ (fast weights)** ç³»ç»Ÿï¼Œå®ƒåœ¨**å†…å¾ªç¯**ä¸­æ ¹æ®â€œæƒŠå¥‡åº¦â€å®æ—¶æ›´æ–°è‡ªå·±çš„å‚æ•° `M`ï¼Œä»¥ç¼–ç åºåˆ—çš„å†å²ä¿¡æ¯ã€‚
    *   **æ ¸å¿ƒæœºåˆ¶**ï¼š
        *   **çŠ¶æ€**ï¼šæŒæœ‰è‡ªèº«å‚æ•° `M`ï¼ˆä¾‹å¦‚ä¸€ä¸ª `MemoryMLP` çš„æƒé‡ï¼‰ã€‚
        *   **è¾“å…¥**ï¼š`f_t`ï¼ˆç”¨äºç”Ÿæˆ k, vï¼‰å’Œä¸Šä¸€æ—¶åˆ»çš„çŠ¶æ€ `M_{t-1}`ã€‚
        *   **æ›´æ–°**ï¼šæ ¹æ®**å…ƒå­¦ä¹ å™¨**æä¾›çš„ `Î¸_t, Î·_t, Î±_t`ï¼Œé€šè¿‡ `torch.func.grad` è®¡ç®—â€œæƒŠå¥‡åº¦â€æ¢¯åº¦ï¼Œå¹¶åº”ç”¨æ›´æ–°è§„åˆ™ `M_t = (1-Î±_t)M_{t-1} + S_t`ã€‚
        *   **è¾“å‡º**ï¼šæ›´æ–°åçš„çŠ¶æ€ `M_t`ï¼Œå¹¶æ ¹æ®æŸ¥è¯¢ `f_t` è¾“å‡ºæ£€ç´¢åˆ°çš„è®°å¿† `m_t`ã€‚
    *   **è®­ç»ƒ**ï¼š**LMMçš„å‚æ•° `M` ä¸ç›´æ¥å‚ä¸å¤–å¾ªç¯çš„æ¢¯åº¦ä¸‹é™**ã€‚å®ƒçš„æ›´æ–°å®Œå…¨ç”±å†…å¾ªç¯çš„â€œæƒŠå¥‡åº¦â€æ¢¯åº¦å’Œå…ƒå‚æ•°é©±åŠ¨ã€‚

3.  **å…ƒå­¦ä¹ å™¨ (Meta-Learner)** (`models/meta_learner.py`)
    *   **èŒè´£**ï¼š**è¿™æ˜¯å®ç°â€œå­¦ä¹ å¦‚ä½•å­¦ä¹ â€çš„å…³é”®**ã€‚å®ƒæ˜¯ä¸€ä¸ªå°å‹ç½‘ç»œï¼Œæ ¹æ®å½“å‰ä¸Šä¸‹æ–‡ `f_t` åŠ¨æ€ç”ŸæˆLMMçš„æ›´æ–°ç­–ç•¥ã€‚
    *   **è¾“å…¥**ï¼šBackboneæå–çš„ç‰¹å¾ `f_t`ã€‚
    *   **è¾“å‡º**ï¼šä¸€ä¸ªå…ƒå‚æ•°å…ƒç»„ `(Î¸_t, Î·_t, Î±_t)`ï¼Œåˆ†åˆ«ä»£è¡¨LMMçš„**å­¦ä¹ ç‡ã€åŠ¨é‡ç³»æ•°å’Œé—å¿˜ç‡**ã€‚
        *   `Î¸_t (å­¦ä¹ ç‡)`: `nn.Linear -> Sigmoid`ï¼Œè¾“å‡ºèŒƒå›´å¦‚ `[1e-5, 1e-2]`ã€‚
        *   `Î·_t (åŠ¨é‡)`: `nn.Linear -> Sigmoid`ï¼Œè¾“å‡ºèŒƒå›´å¦‚ `[0.8, 0.99]`ã€‚
        *   `Î±_t (é—å¿˜ç‡)`: `nn.Linear -> Sigmoid`ï¼Œè¾“å‡ºèŒƒå›´ `[0, 1]`ã€‚
    *   **è®­ç»ƒ**ï¼šå…¶å‚æ•°ç”±**å¤–å¾ªç¯**çš„ `AdamW` ä¼˜åŒ–å™¨æ›´æ–°ã€‚ä¼˜åŒ–ç›®æ ‡æ˜¯ï¼šç”Ÿæˆçš„å…ƒå‚æ•°èƒ½å¦è®©LMMçš„å†…å¾ªç¯æ›´æ–°å¯¹æœ€ç»ˆä»»åŠ¡æŸå¤±æ›´æœ‰åˆ©ã€‚

---

## 3. è®­ç»ƒæµç¨‹ï¼šå†…å¤–åŒå¾ªç¯

è®­ç»ƒæµç¨‹å°†ä¸¥æ ¼éµå¾ªâ€œå…ƒå­¦ä¹ â€èŒƒå¼ï¼Œåˆ†ä¸ºå†…å¤–ä¸¤ä¸ªå¾ªç¯ã€‚

### 3.1 å†…å¾ªç¯ (Inner Loop) - åœ¨çº¿è®°å¿†æ›´æ–°

å†…å¾ªç¯å‘ç”Ÿåœ¨**å‰å‘ä¼ æ’­**è¿‡ç¨‹ä¸­ï¼Œå¯¹åºåˆ—ä¸­çš„æ¯ä¸€æ­¥ `t` æ‰§è¡Œï¼š

```python
# (åœ¨ ContinualForecaster çš„ forward æ–¹æ³•ä¸­)
def forward(self, sequence_x, ...):
    
    # 0. åˆå§‹åŒ–LMMçŠ¶æ€
    memory_state = self.lmm.init_state()

    for t in range(sequence_length):
        x_t = sequence_x[:, t, :]
        
        # 1. Backbone æå–ç‰¹å¾
        f_t = self.backbone(x_t)

        # 2. å…ƒå­¦ä¹ å™¨ç”Ÿæˆå½“å‰æ­¥çš„æ›´æ–°ç­–ç•¥
        meta_params = self.meta_learner(f_t)  # (Î¸_t, Î·_t, Î±_t)

        # 3. LMM æ‰§è¡Œå†…å¾ªç¯æ›´æ–°
        #    æ­¤æ–¹æ³•å†…éƒ¨å®ŒæˆæƒŠå¥‡åº¦è®¡ç®—å’Œå‚æ•°æ›´æ–°
        memory_state = self.lmm.inner_update(f_t, memory_state, meta_params)

        # 4. ä»æ›´æ–°åçš„LMMä¸­æ£€ç´¢è®°å¿†
        memory_output_t = self.lmm.retrieve(f_t, memory_state)
        
        # ... èåˆç‰¹å¾ï¼Œå‡†å¤‡æœ€ç»ˆé¢„æµ‹ ...

    # 5. ä½¿ç”¨åºåˆ—æœ€ç»ˆçŠ¶æ€è¿›è¡Œé¢„æµ‹
    final_prediction = self.prediction_head(...)
    return final_prediction
```

### 3.2 å¤–å¾ªç¯ (Outer Loop) - å…ƒç­–ç•¥ä¼˜åŒ–

å¤–å¾ªç¯æ˜¯æ ‡å‡†çš„ `PyTorch` è®­ç»ƒå¾ªç¯ï¼Œä½†åœ¨**ä¸€æ¬¡å‰å‘ä¼ æ’­**ä¸­å·²ç»å®Œæˆäº†æ•´ä¸ªåºåˆ—çš„**å†…å¾ªç¯**ã€‚

```python
# (åœ¨ Trainer çš„ _train_epoch æ–¹æ³•ä¸­)
for batch_x, batch_y in train_loader:
    
    # 1. æ‰§è¡Œå®Œæ•´çš„å‰å‘ä¼ æ’­ï¼ˆå·²åŒ…å«å†…å¾ªç¯ï¼‰
    #    æ¨¡å‹å†…éƒ¨å¤„ç†å¥½äº†æ•´ä¸ªåºåˆ—çš„LMMçŠ¶æ€æ¼”åŒ–
    y_pred = model(batch_x)

    # 2. è®¡ç®—æœ€ç»ˆä»»åŠ¡æŸå¤±
    #    è¿™ä¸ªæŸå¤±è¯„ä¼°äº†æ•´ä¸ªå†…å¤–å¾ªç¯è¿‡ç¨‹çš„æœ€ç»ˆæ•ˆæœ
    loss = criterion(y_pred, batch_y)

    # 3. åå‘ä¼ æ’­
    #    æ¢¯åº¦ä¼šæµç»ï¼šPrediction Head -> Fusion -> Backbone -> Meta-Learner
    #    å…³é”®ï¼šæ¢¯åº¦ä¸ä¼šç›´æ¥æ›´æ–°LMMçš„å‚æ•°Mï¼Œä½†ä¼šæ›´æ–°æŒ‡å¯¼LMMå­¦ä¹ çš„Meta-Learner
    optimizer.zero_grad()
    loss.backward()

    # 4. æ›´æ–°â€œæ…¢æƒé‡â€
    #    æ›´æ–°Backboneå’ŒMeta-Learnerï¼Œè®©å®ƒä»¬åœ¨ä¸‹ä¸€æ¬¡èƒ½æ›´å¥½åœ°æŒ‡å¯¼LMM
    optimizer.step()
```

---

## 4. ç›®å½•ç»“æ„è®¾è®¡

```
titans_main_v2/
â”œâ”€â”€ titans-pytorch-main/      # æ–°çš„åŸå§‹ä»£ç åº“ (ä½œä¸ºå­æ¨¡å—æˆ–å‚è€ƒ)
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ checkpoints/
â”œâ”€â”€ results/
â”œâ”€â”€ figs/
â”œâ”€â”€ titans_main.py            # ä¸»è¿è¡Œæ–‡ä»¶ (v2)
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ base.yaml             # åŸºç¡€é…ç½®
â”‚   â”œâ”€â”€ exp_baseline.yaml     # å®éªŒ1: æ ‡å‡†åœ¨çº¿å­¦ä¹ 
â”‚   â””â”€â”€ exp_meta_ttt.yaml     # å®éªŒ3: å®Œæ•´å…ƒå­¦ä¹ 
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ framework.py          # æ ¸å¿ƒæ¡†æ¶ (ContinualForecaster)
â”‚   â”œâ”€â”€ backbones.py          # å„ç§Backbone (LSTM, Transformer...)
â”‚   â”œâ”€â”€ memory.py             # ç¥ç»è®°å¿†å•å…ƒ (LMM) çš„å®ç°
â”‚   â””â”€â”€ meta_learner.py       # å…ƒå­¦ä¹ å™¨ (Meta-Learner) çš„å®ç°
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ trainer.py            # è®­ç»ƒå™¨ (v2)ï¼Œå®ç°å†…å¤–åŒå¾ªç¯é€»è¾‘
â”‚   â”œâ”€â”€ data_factory.py
â”‚   â””â”€â”€ ...
â””â”€â”€ NEW_FRAMEWORK_DESIGN.md   # æœ¬è®¾è®¡æ–‡æ¡£
```

---

## 5. å®æ–½è®¡åˆ’ (Roadmap)

### ç¬¬ä¸€é˜¶æ®µï¼šæ­å»ºåŸºç¡€æ¶æ„å’ŒBaseline

1.  **æ­å»ºæ–°ç›®å½•ç»“æ„**ï¼šæŒ‰ç…§ä¸Šè¿°è®¾è®¡åˆ›å»ºæ–‡ä»¶å¤¹å’Œæ–‡ä»¶ã€‚
2.  **å®ç° `Backbone` å’Œ `NoMemoryUnit`**ï¼šè¿ç§»ç°æœ‰ `Backbone` ä»£ç ï¼Œç¡®ä¿ `NoMemoryUnit` æ­£å¸¸å·¥ä½œã€‚
3.  **æ›´æ–° `Trainer` ä»¥æ”¯æŒæ¨¡å¼1 (Baseline)**ï¼š
    *   å®ç°æ ‡å‡†çš„â€œé¢„è®­ç»ƒ + åœ¨çº¿å¾®è°ƒâ€æµç¨‹ã€‚
    *   **ç›®æ ‡**ï¼šå¤ç°**å®éªŒ4**çš„ç»“æœï¼Œç¡®ä¿åŸºç¡€æ¶æ„æ­£ç¡®ã€‚

### ç¬¬äºŒé˜¶æ®µï¼šå®ç°å¸¦å›ºå®šç­–ç•¥çš„LMM (Simple TTT)

1.  **å®ç° `LMM` æ¨¡å— (`models/memory.py`)**ï¼š
    *   å°è£… `titans-pytorch-main` ä¸­çš„ `NeuralMemory`ã€‚
    *   å®ç° `inner_update` æ¥å£ï¼Œä½†æš‚æ—¶ä½¿ç”¨**å›ºå®šçš„è¶…å‚æ•°**ï¼ˆç±»ä¼¼æˆ‘ä»¬ä¹‹å‰çš„åšæ³•ï¼Œä½†è¦æ›´ç¨³å®šï¼‰ã€‚
2.  **æ›´æ–° `Trainer` ä»¥æ”¯æŒæ¨¡å¼2**ï¼š
    *   åœ¨å‰å‘ä¼ æ’­ä¸­åŠ å…¥LMMçš„å†…å¾ªç¯æ›´æ–°ã€‚
    *   **ç›®æ ‡**ï¼šéªŒè¯LMMåœ¨å›ºå®šç­–ç•¥ä¸‹èƒ½å¦å¸¦æ¥æå‡ã€‚å¦‚æœæ­¤æ—¶æ•ˆæœè¿˜ä¸å¦‚Baselineï¼Œè¯´æ˜LMMæœ¬èº«æˆ–å…¶å›ºå®šç­–ç•¥æœ‰é—®é¢˜ã€‚

### ç¬¬ä¸‰é˜¶æ®µï¼šå®ç°å®Œæ•´çš„å…ƒå­¦ä¹ æ¡†æ¶ (Full Meta-TTT)

1.  **å®ç° `Meta-Learner` æ¨¡å— (`models/meta_learner.py`)**ï¼š
    *   æ„å»ºä¸€ä¸ªå°å‹MLPï¼Œè¾“å‡º `(Î¸_t, Î·_t, Î±_t)`ã€‚
    *   æ³¨æ„è¾“å‡ºèŒƒå›´çš„çº¦æŸï¼ˆä½¿ç”¨ `Sigmoid` ç­‰ï¼‰ã€‚
2.  **è”è°ƒæ‰€æœ‰æ¨¡å—**ï¼š
    *   åœ¨ `framework.py` ä¸­å°† `Backbone`, `LMM`, `Meta-Learner` ç»„åˆèµ·æ¥ã€‚
    *   ç¡®ä¿ `Trainer` ä¸­çš„å¤–å¾ªç¯èƒ½å¤Ÿæ­£ç¡®åœ°å°†æ¢¯åº¦ä¼ æ’­åˆ° `Meta-Learner`ã€‚
3.  **è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒå’Œè¯„ä¼°**ï¼š
    *   **ç›®æ ‡**ï¼šè¿è¡Œæ¨¡å¼3ï¼ŒéªŒè¯å®Œæ•´çš„å…ƒå­¦ä¹ æ¡†æ¶æ˜¯å¦èƒ½è¶…è¶ŠBaselineå’ŒSimple TTTï¼Œå®ç°æœ€ä½³æ€§èƒ½ã€‚

é€šè¿‡è¿™ä¸ªåˆ†é˜¶æ®µçš„è®¡åˆ’ï¼Œæˆ‘ä»¬å¯ä»¥ç³»ç»Ÿæ€§åœ°æ„å»ºå’ŒéªŒè¯æ–°æ¡†æ¶ï¼Œæ¯ä¸€æ­¥éƒ½æœ‰æ˜ç¡®çš„ç›®æ ‡å’Œå¯å¯¹æ¯”çš„åŸºçº¿ï¼Œä»è€Œç¨³å¥åœ°è¿ˆå‘æœ€ç»ˆç›®æ ‡ã€‚

---

## 6. æ€§èƒ½ä¼˜åŒ–è®°å½•

### 6.1 å†…å¾ªç¯ä¼˜åŒ– (2025-11-18)

**é—®é¢˜**ï¼š
- åŸå§‹å®ç°ä¸­ï¼Œå†…å¾ªç¯é€ä¸ªtokenè°ƒç”¨backboneï¼Œå¯¼è‡´è®­ç»ƒé€Ÿåº¦ææ…¢
- æ¨¡å¼3 (Full Meta-TTT) è®­ç»ƒä¸€ä¸ªepochéœ€è¦3241ç§’ï¼ˆçº¦54åˆ†é’Ÿï¼‰ï¼Œæ¯”Baselineæ…¢242å€

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š
1. **æ‰¹é‡ç‰¹å¾æå–**ï¼šä¸€æ¬¡æ€§è°ƒç”¨backboneå¤„ç†æ•´ä¸ªåºåˆ— `[batch, seq_len, input_dim]`
2. **æ‰¹é‡å…ƒå‚æ•°ç”Ÿæˆ**ï¼šMeta-Learnerä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªåºåˆ—ï¼Œè¾“å‡º `[batch, seq_len]` çš„å…ƒå‚æ•°
3. **ä¿ç•™é€æ­¥LMMæ›´æ–°**ï¼šLMMçš„çŠ¶æ€æ›´æ–°å¿…é¡»é€æ­¥è¿›è¡Œï¼ˆé€’å½’ä¾èµ–ï¼‰ï¼Œä½†ç‰¹å¾å’Œå…ƒå‚æ•°å·²é¢„è®¡ç®—

**å®ç°ä½ç½®**ï¼š
- `models/framework.py::forward_with_inner_loop()`: æ‰¹é‡æå–ç‰¹å¾å’Œå…ƒå‚æ•°
- `models/meta_learner.py::forward()`: æ”¯æŒåºåˆ—è¾“å…¥ï¼Œè¾“å‡º `[batch, seq_len]` å½¢çŠ¶

**ä¼˜åŒ–æ•ˆæœ**ï¼š
- **æ¨¡å¼2 (Simple TTT)**: å‰2ä¸ªepochä»468.6sé™è‡³464.5sï¼ˆçº¦1%æå‡ï¼‰
  - åŸå› ï¼šæ¨¡å¼2ä¸ä½¿ç”¨Meta-Learnerï¼Œä¼˜åŒ–ä¸»è¦å½±å“backboneè°ƒç”¨
  - LMMçš„é€æ­¥æ›´æ–°ä»æ˜¯ä¸»è¦ç“¶é¢ˆ
- **æ¨¡å¼3 (Full Meta-TTT)**: é¢„æœŸåŠ é€Ÿ10-50å€ï¼ˆå¾…éªŒè¯ï¼‰
  - æ¶ˆé™¤äº†64æ¬¡backboneè°ƒç”¨å’Œ64æ¬¡Meta-Learnerè°ƒç”¨
  - æ”¹ä¸º1æ¬¡æ‰¹é‡è°ƒç”¨

**å‰©ä½™ç“¶é¢ˆ**ï¼š
- LMMçš„`inner_update`å’Œ`retrieve`å¿…é¡»é€æ­¥æ‰§è¡Œï¼ˆçŠ¶æ€é€’å½’ä¾èµ–ï¼‰
- è¿™æ˜¯è®¾è®¡ä¸Šçš„é™åˆ¶ï¼Œæ— æ³•å®Œå…¨å¹¶è¡ŒåŒ–
- æœªæ¥å¯è€ƒè™‘ï¼šå‡å°‘æ›´æ–°é¢‘ç‡ã€ä¼˜åŒ–NeuralMemoryå†…éƒ¨å®ç°

**ä»£ç å˜æ›´**ï¼š
```python
# ä¼˜åŒ–å‰ï¼šé€ä¸ªtokenå¤„ç†
for t in range(seq_len):
    x_t = sequence_x[:, t:t+1, :]
    f_t = self.backbone(x_t)  # 64æ¬¡è°ƒç”¨
    meta_params = self.meta_learner(f_t)  # 64æ¬¡è°ƒç”¨
    memory_state = self.memory_unit.inner_update(f_t, ...)

# ä¼˜åŒ–åï¼šæ‰¹é‡é¢„è®¡ç®—
features_all = self.backbone(sequence_x)  # 1æ¬¡è°ƒç”¨
theta_all, eta_all, alpha_all = self.meta_learner(features_all)  # 1æ¬¡è°ƒç”¨
for t in range(seq_len):
    f_t = features_all[:, t, :]  # ç›´æ¥ç´¢å¼•
    meta_params = (theta_all[:, t], eta_all[:, t], alpha_all[:, t])
    memory_state = self.memory_unit.inner_update(f_t, ...)  # ä»éœ€é€æ­¥æ›´æ–°
```