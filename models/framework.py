"""
Continual Forecasting Framework (v2)

è¿™ä¸ªæ¨¡å—å®šä¹‰äº†åŸºäºå…ƒå­¦ä¹ çš„æŒç»­å­¦ä¹ é¢„æµ‹æ ¸å¿ƒæ¡†æ¶ã€‚
å®ƒå°†Backbone(P)ã€Memory Unit(LMM)å’ŒMeta-Learnerç»„åˆæˆä¸€ä¸ªå®Œæ•´çš„é¢„æµ‹ç³»ç»Ÿã€‚

æ ¸å¿ƒåˆ›æ–°:
1. **å†…å¤–åŒå¾ªç¯**: å†…å¾ªç¯è¿›è¡Œå¿«é€Ÿè®°å¿†æ›´æ–°ï¼Œå¤–å¾ªç¯ä¼˜åŒ–å…ƒç­–ç•¥
2. **å…ƒå­¦ä¹ **: Meta-Learnerå­¦ä¹ å¦‚ä½•åŠ¨æ€è°ƒæ•´LMMçš„æ›´æ–°ç­–ç•¥
3. **æ¨¡å—åŒ–**: ä¸‰å¤§ç»„ä»¶(P, LMM, Meta-Learner)å¯ç‹¬ç«‹æ›¿æ¢

æ”¯æŒä¸‰ç§å®éªŒæ¨¡å¼:
- æ¨¡å¼1 (Baseline): æ ‡å‡†åœ¨çº¿å­¦ä¹ ï¼ˆæ— LMMï¼‰
- æ¨¡å¼2 (Simple TTT): å¸¦LMMï¼Œå›ºå®šæ›´æ–°ç­–ç•¥
- æ¨¡å¼3 (Full Meta-TTT): å¸¦LMMï¼Œå…ƒå­¦ä¹ åŠ¨æ€ç­–ç•¥ï¼ˆç»ˆæç›®æ ‡ï¼‰
"""

import torch
import torch.nn as nn
from typing import Optional, Tuple


class ContinualForecaster(nn.Module):
    """
    æŒç»­å­¦ä¹ æ—¶é—´åºåˆ—é¢„æµ‹å™¨ (v2 - å…ƒå­¦ä¹ ç‰ˆæœ¬)
    
    æ¶æ„æµç¨‹:
        Input [batch, seq_len, input_dim]
          â†“
        Backbone P: ç‰¹å¾æå– â†’ f_t
          â†“
        Meta-Learner: ç”Ÿæˆå…ƒå‚æ•° (Î¸_t, Î·_t, Î±_t)
          â†“
        LMM å†…å¾ªç¯: æ ¹æ®å…ƒå‚æ•°æ›´æ–°è®°å¿† â†’ M_t
          â†“
        Memory Retrieval: æ£€ç´¢è®°å¿† â†’ m_t
          â†“
        Feature Fusion: (f_t + m_t)
          â†“
        Prediction Head: æœ€ç»ˆé¢„æµ‹
          â†“
        Output [batch, pred_len, output_dim]
    
    å‚æ•°:
        backbone: é¢„æµ‹ä¸»å¹²ç½‘ç»œ (P)
        memory_unit: è®°å¿†å•å…ƒ (LMM)
        meta_learner: å…ƒå­¦ä¹ å™¨ (Meta-Learner) - å¯é€‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å›ºå®šç­–ç•¥
        output_dim: è¾“å‡ºç‰¹å¾ç»´åº¦
        pred_len: é¢„æµ‹é•¿åº¦
        fusion_type: ç‰¹å¾èåˆæ–¹å¼ ('add', 'concat', 'gated')
        use_meta_learning: æ˜¯å¦å¯ç”¨å…ƒå­¦ä¹  (æ¨¡å¼3)
    """
    def __init__(
        self,
        backbone: nn.Module,
        memory_unit: nn.Module,
        meta_learner: Optional[nn.Module] = None,
        output_dim: int = 1,
        pred_len: int = 1,
        fusion_type: str = 'add',
        dropout: float = 0.1,
        use_meta_learning: bool = False
    ):
        super().__init__()
        self.backbone = backbone
        self.memory_unit = memory_unit
        self.meta_learner = meta_learner
        self.output_dim = output_dim
        self.pred_len = pred_len
        self.fusion_type = fusion_type
        self.use_meta_learning = use_meta_learning
        
        # è·å–backboneè¾“å‡ºç»´åº¦
        if hasattr(backbone, 'hidden_dim'):
            self.feature_dim = backbone.hidden_dim
        elif hasattr(backbone, 'dim'):
            self.feature_dim = backbone.dim
        else:
            raise ValueError("Backboneå¿…é¡»æœ‰'hidden_dim'æˆ–'dim'å±æ€§")
        
        # ç‰¹å¾èåˆå±‚
        if fusion_type == 'add':
            # ç›´æ¥ç›¸åŠ ï¼Œä¸éœ€è¦é¢å¤–å‚æ•°
            self.fusion = None
            fused_dim = self.feature_dim
        elif fusion_type == 'concat':
            # æ‹¼æ¥åé™ç»´
            self.fusion = nn.Sequential(
                nn.Linear(self.feature_dim * 2, self.feature_dim),
                nn.LayerNorm(self.feature_dim),
                nn.GELU(),
                nn.Dropout(dropout)
            )
            fused_dim = self.feature_dim
        elif fusion_type == 'gated':
            # é—¨æ§èåˆ
            self.fusion = nn.Sequential(
                nn.Linear(self.feature_dim * 2, self.feature_dim),
                nn.Sigmoid()
            )
            fused_dim = self.feature_dim
        else:
            raise ValueError(f"Unknown fusion_type: {fusion_type}")
        
        # é¢„æµ‹å¤´
        self.prediction_head = nn.Sequential(
            nn.Linear(fused_dim, fused_dim),
            nn.LayerNorm(fused_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(fused_dim, output_dim * pred_len)
        )
    
    def forward_with_inner_loop(
        self,
        sequence_x: torch.Tensor,
        cache: Optional[any] = None,
        return_cache: bool = False
    ) -> Tuple[torch.Tensor, Optional[any]]:
        """
        å¸¦å†…å¾ªç¯çš„å‰å‘ä¼ æ’­ - å®ç°è®¾è®¡æ–‡æ¡£ä¸­çš„å†…å¤–åŒå¾ªç¯æœºåˆ¶
        
        å†…å¾ªç¯: å¯¹åºåˆ—ä¸­çš„æ¯ä¸€æ­¥tæ‰§è¡Œè®°å¿†æ›´æ–°
        å¤–å¾ªç¯: é€šè¿‡åå‘ä¼ æ’­ä¼˜åŒ–Backboneå’ŒMeta-Learner
        
        Args:
            sequence_x: [batch, seq_len, input_dim] - è¾“å…¥æ—¶é—´åºåˆ—
            cache: è®°å¿†çŠ¶æ€cache (ç”¨äºæŒç»­å­¦ä¹ )
            return_cache: æ˜¯å¦è¿”å›ä¸‹ä¸€æ­¥çš„cache
        
        Returns:
            pred: [batch, pred_len, output_dim] - é¢„æµ‹ç»“æœ
            next_cache: ä¸‹ä¸€æ­¥çš„cache (å¦‚æœreturn_cache=True)
        """
        batch_size, seq_len = sequence_x.shape[:2]
        
        # 0. åˆå§‹åŒ–LMMçŠ¶æ€
        if cache is None:
            memory_state = self.memory_unit.init_state(batch_size)
        else:
            memory_state = cache
        
        # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šæ‰¹é‡æå–ç‰¹å¾ï¼Œé¿å…é‡å¤è°ƒç”¨Backbone
        # åŸæ¥ï¼šé€ä¸ªtokenè°ƒç”¨backboneï¼ˆæ…¢242å€ï¼‰
        # ä¼˜åŒ–ï¼šä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªåºåˆ—ï¼ˆåŠ é€Ÿ10-50å€ï¼‰
        
        # 1. ä¸€æ¬¡æ€§æå–æ‰€æœ‰ç‰¹å¾ï¼ˆå…³é”®ä¼˜åŒ–ï¼ï¼‰
        features_all = self.backbone(sequence_x)  # [batch, seq_len, feature_dim]
        
        # 2. å¦‚æœå¯ç”¨å…ƒå­¦ä¹ ï¼Œä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰å…ƒå‚æ•°
        if self.use_meta_learning and self.meta_learner is not None:
            # Meta-Learnerå¤„ç†æ•´ä¸ªåºåˆ—ï¼Œè¾“å‡ºæ¯ä¸ªæ—¶é—´æ­¥çš„å…ƒå‚æ•°
            theta_all, eta_all, alpha_all = self.meta_learner(features_all)
            # theta_all: [batch, seq_len], eta_all: [batch, seq_len], alpha_all: [batch, seq_len]
        else:
            theta_all = eta_all = alpha_all = None
        
        # å­˜å‚¨æ¯ä¸€æ­¥çš„è®°å¿†è¾“å‡ºï¼ˆç”¨äºæœ€ç»ˆé¢„æµ‹ï¼‰
        all_memory_outputs = []
        
        # ==== å†…å¾ªç¯ (Inner Loop): é€æ­¥æ›´æ–°LMMçŠ¶æ€ ====
        # æ³¨æ„ï¼šLMMå¿…é¡»é€æ­¥æ›´æ–°ï¼Œå› ä¸ºçŠ¶æ€æ˜¯é€’å½’çš„
        for t in range(seq_len):
            # 3. ç›´æ¥ç´¢å¼•é¢„è®¡ç®—çš„ç‰¹å¾ï¼ˆæ— éœ€å†è°ƒç”¨backboneï¼‰
            f_t = features_all[:, t, :]  # [batch, feature_dim]
            
            # 4. ç´¢å¼•å½“å‰æ—¶é—´æ­¥çš„å…ƒå‚æ•°
            if self.use_meta_learning and self.meta_learner is not None:
                theta_t = theta_all[:, t]  # [batch]
                eta_t = eta_all[:, t]      # [batch]
                alpha_t = alpha_all[:, t]  # [batch]
                meta_params = (theta_t, eta_t, alpha_t)
            else:
                meta_params = None
            
            # 5. LMM æ‰§è¡Œå†…å¾ªç¯æ›´æ–°
            memory_state = self.memory_unit.inner_update(
                f_t, memory_state, meta_params
            )
            
            # 6. ä»æ›´æ–°åçš„LMMä¸­æ£€ç´¢è®°å¿†
            memory_output_t = self.memory_unit.retrieve(f_t, memory_state)  # [batch, feature_dim]
            
            # æ”¶é›†è®°å¿†è¾“å‡º
            all_memory_outputs.append(memory_output_t)
        
        # 8. æ‹¼æ¥ç»“æœï¼ˆfeatures_allå·²ç»æ˜¯æ­£ç¡®å½¢çŠ¶ï¼Œç›´æ¥ä½¿ç”¨ï¼‰
        features = features_all  # [batch, seq_len, feature_dim]
        memory_outputs = torch.stack(all_memory_outputs, dim=1)  # [batch, seq_len, feature_dim]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰nan/inf
        if torch.isnan(features).any() or torch.isinf(features).any():
            raise RuntimeError("Backboneè¾“å‡ºåŒ…å«nan/infï¼")
        if torch.isnan(memory_outputs).any() or torch.isinf(memory_outputs).any():
            raise RuntimeError("Memoryè¾“å‡ºåŒ…å«nan/infï¼")
        
        # 6. ç‰¹å¾èåˆ
        if self.fusion_type == 'add':
            fused = features + memory_outputs
        elif self.fusion_type == 'concat':
            concat_features = torch.cat([features, memory_outputs], dim=-1)
            fused = self.fusion(concat_features)
        elif self.fusion_type == 'gated':
            concat_features = torch.cat([features, memory_outputs], dim=-1)
            gate = self.fusion(concat_features)
            fused = features * gate + memory_outputs * (1 - gate)
        
        # æ£€æŸ¥èåˆåçš„ç‰¹å¾
        if torch.isnan(fused).any() or torch.isinf(fused).any():
            raise RuntimeError("ç‰¹å¾èåˆååŒ…å«nan/infï¼")
        
        # 7. ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„ç‰¹å¾è¿›è¡Œé¢„æµ‹
        last_feature = fused[:, -1, :]  # [batch, feature_dim]
        
        # 8. é¢„æµ‹å¤´
        pred_flat = self.prediction_head(last_feature)  # [batch, output_dim * pred_len]
        pred = pred_flat.view(batch_size, self.pred_len, self.output_dim)
        
        # æ£€æŸ¥æœ€ç»ˆé¢„æµ‹
        if torch.isnan(pred).any() or torch.isinf(pred).any():
            raise RuntimeError("é¢„æµ‹è¾“å‡ºåŒ…å«nan/infï¼")
        
        if return_cache:
            return pred, memory_state
        return pred, None
    
    def forward(
        self,
        x: torch.Tensor,
        cache: Optional[any] = None,
        return_cache: bool = False
    ) -> Tuple[torch.Tensor, Optional[any]]:
        """
        å‰å‘ä¼ æ’­ - è‡ªåŠ¨é€‰æ‹©æ˜¯å¦ä½¿ç”¨å†…å¾ªç¯
        
        Args:
            x: [batch, seq_len, input_dim] - è¾“å…¥æ—¶é—´åºåˆ—
            cache: è®°å¿†çŠ¶æ€cache (ç”¨äºæŒç»­å­¦ä¹ )
            return_cache: æ˜¯å¦è¿”å›ä¸‹ä¸€æ­¥çš„cache
        
        Returns:
            pred: [batch, pred_len, output_dim] - é¢„æµ‹ç»“æœ
            next_cache: ä¸‹ä¸€æ­¥çš„cache (å¦‚æœreturn_cache=True)
        """
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨NoMemoryUnit (æ¨¡å¼1: Baseline)
        is_no_memory = self.memory_unit.__class__.__name__ == 'NoMemoryUnit'
        
        if is_no_memory or not self.use_meta_learning:
            # æ¨¡å¼1æˆ–æ¨¡å¼2: ä½¿ç”¨ç®€åŒ–çš„forward (å…¼å®¹æ—§å®ç°)
            return self._forward_simple(x, cache, return_cache)
        else:
            # æ¨¡å¼3: ä½¿ç”¨å¸¦å†…å¾ªç¯çš„forward
            return self.forward_with_inner_loop(x, cache, return_cache)
    
    def _forward_simple(
        self,
        x: torch.Tensor,
        cache: Optional[any] = None,
        return_cache: bool = False
    ) -> Tuple[torch.Tensor, Optional[any]]:
        """
        ç®€åŒ–çš„å‰å‘ä¼ æ’­ - ç”¨äºæ¨¡å¼1å’Œæ¨¡å¼2
        
        è¿™ä¸ªæ–¹æ³•ä¸åŸæ¥çš„å®ç°å…¼å®¹ï¼Œä¸ä½¿ç”¨é€æ­¥çš„å†…å¾ªç¯ã€‚
        """
        batch_size, seq_len = x.shape[:2]
        
        # 1. Backboneæå–ç‰¹å¾
        features = self.backbone(x)  # [batch, seq_len, feature_dim]
        
        # æ£€æŸ¥backboneè¾“å‡º
        if torch.isnan(features).any() or torch.isinf(features).any():
            raise RuntimeError("Backboneè¾“å‡ºåŒ…å«nan/infï¼")
        
        # 2. Memory Unitå¤„ç†
        try:
            if return_cache:
                memory_output, next_cache = self.memory_unit(
                    features,
                    cache=cache,
                    return_cache=True
                )
            else:
                memory_output, _ = self.memory_unit(
                    features,
                    cache=None,
                    return_cache=False
                )
                next_cache = None
        except RuntimeError as e:
            if "nan" in str(e).lower() or "inf" in str(e).lower():
                raise RuntimeError(f"Memory Unitäº§ç”Ÿnan/inf: {e}")
            raise
        
        # æ£€æŸ¥memoryè¾“å‡º
        if memory_output is None:
            raise RuntimeError("Memory Unitè¾“å‡ºä¸ºNoneï¼")
        if torch.isnan(memory_output).any() or torch.isinf(memory_output).any():
            raise RuntimeError("Memory Unitè¾“å‡ºåŒ…å«nan/infï¼")
        
        # 3. ç‰¹å¾èåˆ
        if self.fusion_type == 'add':
            fused = features + memory_output
        elif self.fusion_type == 'concat':
            concat_features = torch.cat([features, memory_output], dim=-1)
            fused = self.fusion(concat_features)
        elif self.fusion_type == 'gated':
            concat_features = torch.cat([features, memory_output], dim=-1)
            gate = self.fusion(concat_features)
            fused = features * gate + memory_output * (1 - gate)
        
        # æ£€æŸ¥èåˆåçš„ç‰¹å¾
        if torch.isnan(fused).any() or torch.isinf(fused).any():
            raise RuntimeError("ç‰¹å¾èåˆååŒ…å«nan/infï¼")
        
        # 4. ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„ç‰¹å¾è¿›è¡Œé¢„æµ‹
        last_feature = fused[:, -1, :]  # [batch, feature_dim]
        
        # 5. é¢„æµ‹å¤´
        pred_flat = self.prediction_head(last_feature)  # [batch, output_dim * pred_len]
        pred = pred_flat.view(batch_size, self.pred_len, self.output_dim)
        
        # æ£€æŸ¥æœ€ç»ˆé¢„æµ‹
        if torch.isnan(pred).any() or torch.isinf(pred).any():
            raise RuntimeError("é¢„æµ‹è¾“å‡ºåŒ…å«nan/infï¼")
        
        if return_cache:
            return pred, next_cache
        return pred, None
    
    def get_model_info(self):
        """è¿”å›æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯"""
        info = {
            'backbone': self.backbone.__class__.__name__,
            'memory_unit': self.memory_unit.__class__.__name__,
            'meta_learner': self.meta_learner.__class__.__name__ if self.meta_learner else 'None',
            'feature_dim': self.feature_dim,
            'output_dim': self.output_dim,
            'pred_len': self.pred_len,
            'fusion_type': self.fusion_type,
            'use_meta_learning': self.use_meta_learning,
            'total_params': sum(p.numel() for p in self.parameters()),
            'trainable_params': sum(p.numel() for p in self.parameters() if p.requires_grad),
        }
        
        # æ·»åŠ å„ç»„ä»¶çš„é…ç½®
        if hasattr(self.memory_unit, 'get_config'):
            info['memory_config'] = self.memory_unit.get_config()
        
        if self.meta_learner and hasattr(self.meta_learner, 'get_config'):
            info['meta_learner_config'] = self.meta_learner.get_config()
        
        return info


def build_continual_forecaster(
    backbone_type: str,
    memory_type: str,
    input_dim: int,
    output_dim: int,
    pred_len: int,
    seq_len: int = 64,
    backbone_dim: int = 384,
    backbone_depth: int = 4,
    backbone_heads: int = 6,
    neural_memory_batch_size: int = 256,
    memory_chunk_size: int = 1,
    memory_model_type: str = 'mlp',
    fusion_type: str = 'add',
    # å…ƒå­¦ä¹ ç›¸å…³å‚æ•°
    use_meta_learning: bool = False,
    meta_learner_type: str = 'fixed',  # 'adaptive' or 'fixed'
    meta_learner_hidden_dim: int = 128,
    **kwargs
) -> ContinualForecaster:
    """
    å·¥å‚å‡½æ•°ï¼šæ„å»ºå®Œæ•´çš„æŒç»­å­¦ä¹ é¢„æµ‹å™¨
    
    Args:
        backbone_type: 'lstm', 'transformer', 'titans'
        memory_type: 'lmm_mlp', 'lmm_attention', 'none'
        input_dim: è¾“å…¥ç‰¹å¾ç»´åº¦
        output_dim: è¾“å‡ºç‰¹å¾ç»´åº¦
        pred_len: é¢„æµ‹é•¿åº¦
        seq_len: è¾“å…¥åºåˆ—é•¿åº¦
        backbone_dim: Backboneçš„éšè—ç»´åº¦
        use_meta_learning: æ˜¯å¦å¯ç”¨å…ƒå­¦ä¹  (æ¨¡å¼3)
        meta_learner_type: å…ƒå­¦ä¹ å™¨ç±»å‹ ('adaptive'æˆ–'fixed')
        å…¶ä»–é…ç½®å‚æ•°...
    
    Returns:
        model: ContinualForecaster
    """
    from models.backbones import build_backbone
    from models.memory import build_memory_unit
    from models.meta_learner import build_meta_learner
    
    # æ„å»ºBackbone
    backbone_kwargs = {
        'input_dim': input_dim,
    }
    
    if backbone_type == 'lstm':
        backbone_kwargs.update({
            'hidden_dim': backbone_dim,
            'num_layers': backbone_depth,
        })
    elif backbone_type == 'transformer':
        backbone_kwargs.update({
            'dim': backbone_dim,
            'depth': backbone_depth,
            'heads': backbone_heads,
        })
    elif backbone_type == 'titans':
        backbone_kwargs.update({
            'dim': backbone_dim,
            'depth': backbone_depth,
            'heads': backbone_heads,
            'seq_len': seq_len,
        })
    
    backbone = build_backbone(backbone_type, **backbone_kwargs)
    
    # æ„å»ºMemory Unit
    memory_kwargs = {
        'dim': backbone_dim,
        'chunk_size': memory_chunk_size,
        'neural_memory_batch_size': neural_memory_batch_size,
    }
    
    if memory_type.startswith('lmm') or memory_type.startswith('titans'):
        memory_kwargs.update({
            'heads': 1,
            'mlp_depth': 2,
            'mlp_expansion_factor': 4.0,
            'max_grad_norm': 0.5,
            'default_step_transform_max_lr': 0.001,
            'init_adaptive_step_bias': -15.0,
            'momentum': False,
            'qk_rmsnorm': False,
            'attn_pool_chunks': False,
        })
    
    memory_unit = build_memory_unit(memory_type, **memory_kwargs)
    
    # æ„å»ºMeta-Learner (å¦‚æœå¯ç”¨å…ƒå­¦ä¹ )
    meta_learner = None
    if use_meta_learning and memory_type != 'none':
        meta_kwargs = {
            'meta_learner_type': meta_learner_type,
            'input_dim': backbone_dim,
        }
        if meta_learner_type.lower() == 'adaptive':
            meta_kwargs['hidden_dim'] = meta_learner_hidden_dim
        meta_learner = build_meta_learner(**meta_kwargs)
    
    # æ„å»ºå®Œæ•´æ¨¡å‹
    model = ContinualForecaster(
        backbone=backbone,
        memory_unit=memory_unit,
        meta_learner=meta_learner,
        output_dim=output_dim,
        pred_len=pred_len,
        fusion_type=fusion_type,
        use_meta_learning=use_meta_learning
    )
    
    return model


if __name__ == '__main__':
    print("æµ‹è¯•æŒç»­å­¦ä¹ é¢„æµ‹æ¡†æ¶...")
    
    # æµ‹è¯•æ¨¡å¼1: Baseline (æ— Memory)
    print("\n1. æµ‹è¯•æ¨¡å¼1: Baseline")
    model1 = build_continual_forecaster(
        backbone_type='lstm',
        memory_type='none',
        input_dim=3,
        output_dim=1,
        pred_len=1,
        seq_len=64,
        backbone_dim=256,
        use_meta_learning=False
    )
    x = torch.randn(2, 64, 3)
    pred1, _ = model1(x)
    print(f"   æ¨¡å‹: {model1.get_model_info()['backbone']} + {model1.get_model_info()['memory_unit']}")
    print(f"   è¾“å‡ºå½¢çŠ¶: {pred1.shape}")
    
    # æµ‹è¯•æ¨¡å¼2: Simple TTT (Fixedç­–ç•¥)
    print("\n2. æµ‹è¯•æ¨¡å¼2: Simple TTT")
    model2 = build_continual_forecaster(
        backbone_type='transformer',
        memory_type='lmm_mlp',
        input_dim=3,
        output_dim=1,
        pred_len=1,
        seq_len=64,
        backbone_dim=256,
        use_meta_learning=False,
        meta_learner_type='fixed'
    )
    pred2, _ = model2(x)
    print(f"   æ¨¡å‹: {model2.get_model_info()['backbone']} + {model2.get_model_info()['memory_unit']}")
    print(f"   è¾“å‡ºå½¢çŠ¶: {pred2.shape}")
    
    # æµ‹è¯•æ¨¡å¼3: Full Meta-TTT (è‡ªé€‚åº”ç­–ç•¥)
    print("\n3. æµ‹è¯•æ¨¡å¼3: Full Meta-TTT")
    model3 = build_continual_forecaster(
        backbone_type='titans',
        memory_type='lmm_mlp',
        input_dim=3,
        output_dim=1,
        pred_len=1,
        seq_len=64,
        backbone_dim=256,
        use_meta_learning=True,
        meta_learner_type='adaptive'
    )
    pred3, _ = model3(x)
    print(f"   æ¨¡å‹: {model3.get_model_info()['backbone']} + {model3.get_model_info()['memory_unit']} + {model3.get_model_info()['meta_learner']}")
    print(f"   è¾“å‡ºå½¢çŠ¶: {pred3.shape}")
    
    print("\nâœ“ æ¡†æ¶æµ‹è¯•é€šè¿‡!")
