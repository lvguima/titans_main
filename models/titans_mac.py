"""
Titans MAC (Memory-As-Context) æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹
åŸºäºtitans_pytorchå°è£…çš„è‡ªå®šä¹‰ç‰ˆæœ¬ï¼Œé€‚é…æ—¶é—´åºåˆ—é¢„æµ‹ä»»åŠ¡
"""

import torch
import torch.nn as nn
import sys
from pathlib import Path

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥titans_pytorch
parent_dir = Path(__file__).parent.parent
titans_pytorch_path = parent_dir / 'titans-pytorch-original'
sys.path.insert(0, str(titans_pytorch_path))

from titans_pytorch import MemoryAsContextTransformer, MemoryAttention


class TimeSeriesTitansTransformer(nn.Module):
    """
    è‡ªå®šä¹‰æ—¶é—´åºåˆ—Titans Transformer
    å°†è¿ç»­æ—¶é—´åºåˆ—è¾“å…¥é€‚é…åˆ°MAC Transformeræ¶æ„
    """
    
    def __init__(
        self, 
        input_dim,
        output_dim,
        pred_len,
        dim,
        depth,
        segment_len,
        num_persist_mem_tokens=0,
        num_longterm_mem_tokens=0,
        neural_memory_layers=None,
        neural_memory_segment_len=None,
        neural_memory_batch_size=None,
        neural_mem_weight_residual=False,
        neural_mem_gate_attn_output=False,
        use_flex_attn=False,
        sliding_window_attn=False,
        dim_head=64,
        heads=8,
        neural_memory_model=None,
        neural_memory_kwargs=None,
    ):
        super().__init__()
        
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.pred_len = pred_len
        self.dim = dim
        
        # 1. è¾“å…¥æŠ•å½±å±‚ï¼ˆåœ¨transformerå¤–éƒ¨å¤„ç†ï¼‰
        self.input_projection = nn.Linear(input_dim, dim)
        
        # 2. åˆ›å»ºç‰¹æ®Šçš„token_embæ¥å¤„ç†å·²æŠ•å½±çš„æ•°æ®
        # åŸå§‹åº“æœŸæœ›è¾“å…¥æ˜¯2Dçš„[batch, seq_len]ï¼Œç„¶åtoken_embè½¬ä¸º3D
        # æˆ‘ä»¬çš„ç­–ç•¥ï¼šåœ¨forwardä¸­å°†3DæŠ•å½±åçš„æ•°æ®flattenæˆ2Dä¼ å…¥ï¼Œ
        # ç„¶ååœ¨è¿™ä¸ªç‰¹æ®Šçš„embeddingä¸­reshapeå›3D
        class ReshapeEmbedding(nn.Module):
            """
            ç‰¹æ®Šçš„embeddingï¼Œç”¨äºå¤„ç†å·²ç»æŠ•å½±å¥½çš„3Dæ•°æ®
            è¾“å…¥ï¼š[batch*seq_len, dim] (è¢«flattençš„3Dæ•°æ®)
            è¾“å‡ºï¼š[batch, seq_len, dim] (æ¢å¤3Då½¢çŠ¶)
            """
            def __init__(self):
                super().__init__()
                self.stored_shape = None  # ç”¨äºå­˜å‚¨åŸå§‹å½¢çŠ¶
            
            def forward(self, x):
                # x: [batch, seq_len] ä½†å®é™…æ˜¯è¢«reshapeçš„ [batch*seq_len, dim]çš„ç¬¬ä¸€ç»´
                # æˆ‘ä»¬éœ€è¦ä»stored_shapeæ¢å¤
                if self.stored_shape is not None:
                    batch, seq_len, dim = self.stored_shape
                    # xè¿™é‡Œå…¶å®æ˜¯ä¸ªdummyï¼Œæˆ‘ä»¬ç›´æ¥è¿”å›stored_data
                    result = self.stored_data
                    self.stored_shape = None  # æ¸…ç©º
                    return result
                else:
                    # ä¸åº”è¯¥åˆ°è¿™é‡Œ
                    raise RuntimeError("ReshapeEmbedding: stored_shape not set")
        
        self.reshape_emb = ReshapeEmbedding()
        
        self.transformer = MemoryAsContextTransformer(
            num_tokens=dim,  # è®¾ä¸ºdimï¼Œè¿™æ ·to_logitsè¾“å‡º[batch, seq_len, dim]è€Œä¸æ˜¯[batch, seq_len, 1]
            dim=dim,
            depth=depth,
            segment_len=segment_len,
            num_persist_mem_tokens=num_persist_mem_tokens,
            num_longterm_mem_tokens=num_longterm_mem_tokens,
            neural_memory_layers=neural_memory_layers,
            neural_memory_segment_len=neural_memory_segment_len,
            neural_memory_batch_size=neural_memory_batch_size,
            neural_mem_weight_residual=neural_mem_weight_residual,
            neural_mem_gate_attn_output=neural_mem_gate_attn_output,
            use_flex_attn=use_flex_attn,
            sliding_window_attn=sliding_window_attn,
            dim_head=dim_head,
            heads=heads,
            neural_memory_model=neural_memory_model,
            neural_memory_kwargs=neural_memory_kwargs or {},
            token_emb=self.reshape_emb,  # ä½¿ç”¨ç‰¹æ®Šçš„reshape embedding
        )
        
        # 3. è¾“å‡ºé¢„æµ‹å¤´ï¼šæ¨¡å‹ç»´åº¦ â†’ é¢„æµ‹å€¼
        self.prediction_head = nn.Linear(dim, output_dim * pred_len)
    
    def forward(self, x, cache=None, return_cache=False):
        """
        Args:
            x: è¾“å…¥åºåˆ— [batch, seq_len, input_dim] (è¿ç»­ç‰¹å¾)
            cache: NeuralMemoryç¼“å­˜çŠ¶æ€ (seq_index, kv_caches, neural_mem_caches)
            return_cache: æ˜¯å¦è¿”å›æ›´æ–°åçš„cache
        
        Returns:
            å¦‚æœreturn_cache=False: é¢„æµ‹å€¼ [batch, pred_len, output_dim]
            å¦‚æœreturn_cache=True: (é¢„æµ‹å€¼, æ›´æ–°åçš„cache)
        """
        # Step 1: æŠ•å½±è¿ç»­ç‰¹å¾åˆ°æ¨¡å‹ç»´åº¦
        # x: [batch, seq_len, input_dim] â†’ [batch, seq_len, dim]
        batch, seq_len = x.shape[:2]
        x_proj = self.input_projection(x)  # [batch, seq_len, dim]
        
        # Step 2: åŸå§‹åº“æœŸæœ›2Dè¾“å…¥[batch, seq_len]ï¼Œæˆ‘ä»¬éœ€è¦"ä¼ªè£…"
        # å°†æŠ•å½±åçš„3Dæ•°æ®å­˜å‚¨åˆ°reshape_embä¸­ï¼Œç„¶åä¼ å…¥ä¸€ä¸ªdummyçš„2Dç´¢å¼•
        self.reshape_emb.stored_data = x_proj
        self.reshape_emb.stored_shape = (batch, seq_len, self.dim)
        
        # åˆ›å»ºdummyçš„2Dè¾“å…¥ï¼ˆåŸå§‹åº“ä¼šåœ¨Line 717è§£åŒ…è¿™ä¸ªçš„shapeï¼‰
        x_dummy = torch.zeros(batch, seq_len, dtype=torch.long, device=x.device)
        
        # Step 3: MAC Transformerå‰å‘ä¼ æ’­
        if return_cache:
            # åœ¨çº¿å­¦ä¹ æ¨¡å¼ï¼šéœ€è¦ç»´æŠ¤cacheçŠ¶æ€
            logits, next_cache = self.transformer(
                x_dummy,  # ä¼ å…¥2D dummyï¼Œtoken_embä¼šä»stored_dataä¸­å–å‡ºçœŸå®çš„3Dæ•°æ®
                cache=cache, 
                return_cache=True,
                disable_flex_attn=True  # æ—¶é—´åºåˆ—é€šå¸¸ä¸éœ€è¦flex attention
            )
            
            # å¤„ç†longterm_mem tokençš„ç‰¹æ®Šæƒ…å†µï¼ˆåŸå§‹åº“åœ¨æŸäº›ä½ç½®è¿”å›Noneï¼‰
            if logits is None:
                # å½“å¤„ç†longterm_mem tokensæ—¶ï¼Œç›´æ¥è¿”å›cache
                return None, next_cache
        else:
            # æ­£å¸¸è®­ç»ƒ/æ¨ç†æ¨¡å¼
            logits = self.transformer(x_dummy, disable_flex_attn=True)
            next_cache = None
        
        # Step 3: æå–æœ€åä¸€ä¸ªtokençš„è¡¨ç¤º
        # logits shape: [batch, seq_len, dim]
        last_hidden = logits[:, -1, :]  # [batch, dim]
        
        # Step 4: é¢„æµ‹
        pred = self.prediction_head(last_hidden)  # [batch, output_dim * pred_len]
        pred = pred.view(-1, self.pred_len, self.output_dim)  # [batch, pred_len, output_dim]
        
        if return_cache:
            return pred, next_cache
        return pred


class TitansMAC(nn.Module):
    """
    Titans MACæ¨¡å‹åŒ…è£…å™¨
    æä¾›çµæ´»çš„è¾“å…¥è¾“å‡ºç»´åº¦é…ç½®ï¼Œé€‚é…ä¸åŒçš„æ—¶é—´åºåˆ—æ•°æ®é›†
    """
    
    def __init__(self, args):
        """
        åˆå§‹åŒ–Titans MACæ¨¡å‹
        
        Args:
            args: å‚æ•°å¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰é…ç½®
        """
        super(TitansMAC, self).__init__()
        
        self.args = args
        self.input_dim = args.input_dim
        self.output_dim = args.output_dim
        self.pred_len = args.pred_len
        
        # åˆ›å»ºç¥ç»è®°å¿†æ¨¡å‹
        neural_memory_model = self._create_memory_model(args)
        
        # è¡¥å…¨neural_memory_kwargsï¼ˆæ·»åŠ åŸå§‹åº“ä¸­ä½¿ç”¨çš„é‡è¦å‚æ•°ï¼‰
        neural_memory_kwargs = {
            'dim_head': args.memory_dim_head,
            'heads': args.memory_heads,
            'momentum': args.memory_momentum,
            'momentum_order': args.memory_momentum_order,
            'default_step_transform_max_lr': args.memory_max_lr,
            'use_accelerated_scan': args.memory_use_accelerated_scan,
            # ğŸ”‘ æ–°å¢ï¼šåŸå§‹å®éªŒä¸­è¯æ˜æœ‰æ•ˆçš„å‚æ•°
            'attn_pool_chunks': True,  # ä½¿ç”¨æ³¨æ„åŠ›æ± åŒ–chunk representations
            'qk_rmsnorm': True,  # QKå½’ä¸€åŒ–ï¼Œç¨³å®šè®­ç»ƒ
            'per_parameter_lr_modulation': True,  # æ¯å±‚å­¦ä¹ ç‡è°ƒåˆ¶
            'spectral_norm_surprises': True,  # æ¢¯åº¦è°±å½’ä¸€åŒ–ï¼ˆMuonä¼˜åŒ–å™¨å¯å‘ï¼‰
        }
        
        # åˆ›å»ºè‡ªå®šä¹‰æ—¶é—´åºåˆ—Titansæ¨¡å‹
        self.model = TimeSeriesTitansTransformer(
            input_dim=args.input_dim,
            output_dim=args.output_dim,
            pred_len=args.pred_len,
            dim=args.dim,
            depth=args.depth,
            segment_len=args.segment_len,
            num_persist_mem_tokens=args.num_persist_mem_tokens,
            num_longterm_mem_tokens=args.num_longterm_mem_tokens,
            neural_memory_layers=args.neural_memory_layers,
            neural_memory_segment_len=args.neural_memory_segment_len,
            neural_memory_batch_size=args.neural_memory_batch_size,
            neural_mem_weight_residual=args.neural_mem_weight_residual,
            neural_mem_gate_attn_output=not args.use_mac_fusion,  # MACæ¨¡å¼ä¸‹ä¸ºFalse
            use_flex_attn=args.use_flex_attn,
            sliding_window_attn=args.sliding_window_attn,
            dim_head=args.dim_head,
            heads=args.heads,
            neural_memory_model=neural_memory_model,
            neural_memory_kwargs=neural_memory_kwargs,
        )
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        self._print_model_info()
        
        # ğŸ” è°ƒè¯•ï¼šæ‰“å°NeuralMemoryå±‚çš„é…ç½®
        self._debug_neural_memory()
    
    def _create_memory_model(self, args):
        """åˆ›å»ºç¥ç»è®°å¿†æ¨¡å‹"""
        from titans_pytorch import (
            MemoryAttention, 
            MemoryMLP, 
            FactorizedMemoryMLP, 
            MemorySwiGluMLP, 
            GatedResidualMemoryMLP
        )
        
        # æ ¹æ®ä¸åŒçš„è®°å¿†æ¨¡å‹ç±»å‹åˆ›å»ºï¼ˆæ¯ç§æ¨¡å‹çš„å‚æ•°ä¸åŒï¼‰
        if args.memory_model_type == 'attention':
            # MemoryAttention(dim, scale, expansion_factor)
            return MemoryAttention(
                dim=args.memory_dim,
                scale=args.memory_scale,
                expansion_factor=args.memory_expansion_factor
            )
        elif args.memory_model_type == 'mlp':
            # MemoryMLP(dim, depth, expansion_factor)
            return MemoryMLP(
                dim=args.memory_dim,
                depth=2,  # é»˜è®¤2å±‚
                expansion_factor=args.memory_expansion_factor
            )
        elif args.memory_model_type == 'factorized_mlp':
            # FactorizedMemoryMLP(dim, depth, expansion_factor)
            return FactorizedMemoryMLP(
                dim=args.memory_dim,
                depth=2,
                expansion_factor=args.memory_expansion_factor
            )
        elif args.memory_model_type == 'swiglu_mlp':
            # MemorySwiGluMLP(dim, depth=1, expansion_factor)
            return MemorySwiGluMLP(
                dim=args.memory_dim,
                depth=1,  # é»˜è®¤1ï¼ˆSwiGLUè®ºæ–‡æ¨èï¼‰
                expansion_factor=args.memory_expansion_factor
            )
        elif args.memory_model_type == 'gated_residual':
            # GatedResidualMemoryMLP(dim, depth, k=32)
            return GatedResidualMemoryMLP(
                dim=args.memory_dim,
                depth=2,
                k=32
            )
        else:
            # é»˜è®¤ä½¿ç”¨MLP
            return MemoryMLP(
                dim=args.memory_dim,
                depth=2,
                expansion_factor=args.memory_expansion_factor
            )
    
    def _print_model_info(self):
        """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        print(f'\n{"="*70}')
        print(f"Titans MAC æ¨¡å‹ä¿¡æ¯:")
        print(f"{'='*70}")
        print(f"  è¾“å…¥ç»´åº¦: {self.input_dim}")
        print(f"  è¾“å‡ºç»´åº¦: {self.output_dim}")
        print(f"  é¢„æµ‹é•¿åº¦: {self.pred_len}")
        print(f"  æ¨¡å‹ç»´åº¦: {self.args.dim}")
        print(f"  å±‚æ•°: {self.args.depth}")
        print(f"  æ³¨æ„åŠ›å¤´æ•°: {self.args.heads}")
        print(f"  ç¥ç»è®°å¿†å±‚: {self.args.neural_memory_layers}")
        print(f"  æ€»å‚æ•°é‡: {total_params:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"{'='*70}\n")
    
    def _debug_neural_memory(self):
        """è°ƒè¯•ï¼šæ£€æŸ¥NeuralMemoryæ˜¯å¦è¢«æ­£ç¡®åˆ›å»º"""
        print("="*70)
        print("ğŸ” NeuralMemoryè°ƒè¯•ä¿¡æ¯:")
        print("="*70)
        
        # æ£€æŸ¥transformerçš„layers
        has_neural_mem = False
        for i, layer_modules in enumerate(self.model.transformer.layers):
            mem = layer_modules[4]  # memåœ¨ç¬¬5ä¸ªä½ç½®ï¼ˆindex=4ï¼‰
            if mem is not None:
                has_neural_mem = True
                batch_size = mem.batch_size if hasattr(mem, 'batch_size') else 'None'
                chunk_size = mem.chunk_size if hasattr(mem, 'chunk_size') else 'None'
                print(f"  Layer {i+1}: âœ… æœ‰NeuralMemory")
                print(f"    - batch_size: {batch_size}")
                print(f"    - chunk_size: {chunk_size}")
                print(f"    - è®°å¿†å‚æ•°: {sum(p.numel() for p in mem.parameters()):,}")
        
        if not has_neural_mem:
            print("  âš ï¸ è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°ä»»ä½•NeuralMemoryå±‚ï¼")
        
        print("="*70 + "\n")
    
    def forward(self, x_enc, x_mark_enc=None, x_dec=None, x_mark_dec=None, cache=None, return_cache=False):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x_enc: è¾“å…¥åºåˆ— [batch_size, seq_len, input_dim]
            x_mark_enc: æ—¶é—´ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
            x_dec: decoderè¾“å…¥ï¼ˆå¯é€‰ï¼ŒæŸäº›æ¶æ„éœ€è¦ï¼‰
            x_mark_dec: decoderæ—¶é—´ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
            cache: NeuralMemoryç¼“å­˜çŠ¶æ€ (seq_index, kv_caches, neural_mem_caches)ï¼Œç”¨äºåœ¨çº¿å­¦ä¹ 
            return_cache: æ˜¯å¦è¿”å›æ›´æ–°åçš„cacheçŠ¶æ€
        
        Returns:
            å¦‚æœreturn_cache=False: è¾“å‡ºé¢„æµ‹ [batch_size, pred_len, output_dim]
            å¦‚æœreturn_cache=True: (è¾“å‡ºé¢„æµ‹, æ›´æ–°åçš„cache)
        """
        # ğŸ”‘ å…³é”®ï¼šå°†cacheå‚æ•°ä¼ é€’ç»™åº•å±‚TimeSeriesTitansTransformer
        if return_cache:
            # åœ¨çº¿å­¦ä¹ æ¨¡å¼ï¼šéœ€è¦ç»´æŠ¤cacheçŠ¶æ€
            output, next_cache = self.model(x_enc, cache=cache, return_cache=True)
            
            # è°ƒæ•´è¾“å‡ºç»´åº¦
            if self.pred_len == 1 and output.dim() == 2:
                output = output.unsqueeze(1)  # [batch_size, 1, output_dim]
            
            return output, next_cache
        else:
            # æ­£å¸¸è®­ç»ƒ/æ¨ç†ï¼šä¸ä½¿ç”¨cache
            output = self.model(x_enc)
            
            # è°ƒæ•´è¾“å‡ºç»´åº¦
            if self.pred_len == 1 and output.dim() == 2:
                output = output.unsqueeze(1)  # [batch_size, 1, output_dim]
            elif output.dim() == 2:
                output = output.unsqueeze(1)
            
            return output
    
    def freeze_non_memory_params(self):
        """
        å†»ç»“éè®°å¿†å‚æ•°ï¼ˆbackbone/é¢„æµ‹ç»“æ„ï¼‰ï¼Œä»…ä¿ç•™è®°å¿†å‚æ•°å¯è®­ç»ƒ
        ç”¨äºåœ¨çº¿å­¦ä¹ æ—¶åªæ›´æ–°è®°å¿†å•å…ƒ
        """
        for name, param in self.named_parameters():
            if 'neural_memory' in name or 'mem' in name or 'longterm_mems' in name or 'persist_mem' in name:
                param.requires_grad = True
            else:
                param.requires_grad = False
        
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.parameters())
        print(f"âœ“ å·²å†»ç»“éè®°å¿†å‚æ•°")
        print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.1f}%)")
    
    def unfreeze_all(self):
        """è§£å†»æ‰€æœ‰å‚æ•°ï¼ˆbackbone + è®°å¿†ï¼‰"""
        for param in self.parameters():
            param.requires_grad = True
        
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"âœ“ å·²è§£å†»æ‰€æœ‰å‚æ•°: {trainable_params:,}")
    
    def get_memory_params(self):
        """è·å–è®°å¿†å•å…ƒçš„å‚æ•°ï¼ˆç”¨äºå•ç‹¬ä¼˜åŒ–ï¼‰"""
        return [p for name, p in self.named_parameters() 
                if ('neural_memory' in name or 'mem' in name) and p.requires_grad]
    
    def get_backbone_params(self):
        """è·å–backbone/é¢„æµ‹ç»“æ„çš„å‚æ•°"""
        return [p for name, p in self.named_parameters() 
                if not ('neural_memory' in name or 'mem' in name) and p.requires_grad]


def build_model(args):
    """
    æ„å»ºæ¨¡å‹çš„å·¥å‚å‡½æ•°
    
    Args:
        args: å‚æ•°é…ç½®
    
    Returns:
        æ¨¡å‹å®ä¾‹
    """
    model = TitansMAC(args)
    return model


if __name__ == '__main__':
    """æµ‹è¯•æ¨¡å‹"""
    import argparse
    
    # åˆ›å»ºæµ‹è¯•å‚æ•°
    args = argparse.Namespace()
    
    # åŸºç¡€é…ç½®
    args.input_dim = 7
    args.output_dim = 7
    args.pred_len = 1
    
    # æ¨¡å‹é…ç½®
    args.dim = 256
    args.depth = 4
    args.segment_len = 16
    args.dim_head = 64
    args.heads = 4
    args.dropout = 0.1
    
    # è®°å¿†é…ç½®
    args.num_persist_mem_tokens = 4
    args.num_longterm_mem_tokens = 4
    args.neural_memory_layers = (1, 3)
    args.neural_memory_segment_len = 8
    args.neural_memory_batch_size = 32
    args.neural_mem_weight_residual = True
    
    # è®°å¿†æ¨¡å‹é…ç½®
    args.memory_model_type = 'attention'
    args.memory_dim = 64
    args.memory_scale = 8.0
    args.memory_expansion_factor = 2
    args.memory_dim_head = 64
    args.memory_heads = 4
    args.memory_momentum = True
    args.memory_momentum_order = 2
    args.memory_max_lr = 0.0001
    args.memory_use_accelerated_scan = False
    
    # MACé…ç½®
    args.use_mac_fusion = True
    args.use_flex_attn = False
    args.sliding_window_attn = False
    
    # åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºTitans MACæ¨¡å‹...")
    model = TitansMAC(args)
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size = 8
    seq_len = 96
    x = torch.randn(batch_size, seq_len, args.input_dim)
    
    print(f"\nè¾“å…¥å½¢çŠ¶: {x.shape}")
    output = model(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    print("\nâœ“ æ¨¡å‹æµ‹è¯•é€šè¿‡!")

