"""
æµ‹è¯•æ–°æ¡†æ¶çš„åŠŸèƒ½
éªŒè¯å„ä¸ªæ¨¡å—æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import torch
import sys
import os

# æ·»åŠ titans-pytorch-originalåˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'titans-pytorch-original'))

from models.backbones import build_backbone, LSTMBackbone, TransformerBackbone, TitansBackbone
from models.memory_units import build_memory_unit, TitansMemoryWrapper, NoMemoryUnit
from models.framework import ContinualForecaster, build_continual_forecaster


def test_backbones():
    """æµ‹è¯•æ‰€æœ‰Backbone"""
    print("\n" + "="*70)
    print("æµ‹è¯• Backbones")
    print("="*70)
    
    batch_size = 4
    seq_len = 32
    input_dim = 3
    
    x = torch.randn(batch_size, seq_len, input_dim)
    
    # æµ‹è¯•LSTM
    print("\n1. æµ‹è¯• LSTMBackbone...")
    lstm_backbone = build_backbone('lstm', input_dim=input_dim, hidden_dim=128)
    lstm_out = lstm_backbone(x)
    print(f"   è¾“å…¥: {x.shape}")
    print(f"   è¾“å‡º: {lstm_out.shape}")
    print(f"   âœ“ LSTMBackbone å·¥ä½œæ­£å¸¸")
    
    # æµ‹è¯•Transformer
    print("\n2. æµ‹è¯• TransformerBackbone...")
    transformer_backbone = build_backbone('transformer', input_dim=input_dim, dim=128, depth=2, heads=4)
    transformer_out = transformer_backbone(x)
    print(f"   è¾“å…¥: {x.shape}")
    print(f"   è¾“å‡º: {transformer_out.shape}")
    print(f"   âœ“ TransformerBackbone å·¥ä½œæ­£å¸¸")
    
    # æµ‹è¯•Titans
    print("\n3. æµ‹è¯• TitansBackbone...")
    titans_backbone = build_backbone('titans', input_dim=input_dim, dim=128, depth=2, heads=4, seq_len=seq_len)
    titans_out = titans_backbone(x)
    print(f"   è¾“å…¥: {x.shape}")
    print(f"   è¾“å‡º: {titans_out.shape}")
    print(f"   âœ“ TitansBackbone å·¥ä½œæ­£å¸¸")
    
    print("\n" + "="*70)


def test_memory_units():
    """æµ‹è¯•æ‰€æœ‰Memory Unit"""
    print("\n" + "="*70)
    print("æµ‹è¯• Memory Units")
    print("="*70)
    
    batch_size = 4
    seq_len = 32
    dim = 128
    
    features = torch.randn(batch_size, seq_len, dim)
    
    # æµ‹è¯•TitansMemory (MLP)
    print("\n1. æµ‹è¯• TitansMemoryWrapper (MLP)...")
    memory_mlp = build_memory_unit('titans_mlp', dim=dim, neural_memory_batch_size=256)
    
    # ä¸ä½¿ç”¨cache
    mem_out1, _ = memory_mlp(features, cache=None, return_cache=False)
    print(f"   è¾“å…¥: {features.shape}")
    print(f"   è¾“å‡º: {mem_out1.shape}")
    
    # ä½¿ç”¨cache
    mem_out2, cache = memory_mlp(features, cache=None, return_cache=True)
    print(f"   è¾“å‡ºï¼ˆwith cacheï¼‰: {mem_out2.shape}")
    print(f"   Cacheç±»å‹: {type(cache)}")
    print(f"   âœ“ TitansMemoryWrapper (MLP) å·¥ä½œæ­£å¸¸")
    
    # æµ‹è¯•NoMemory
    print("\n2. æµ‹è¯• NoMemoryUnit...")
    no_memory = build_memory_unit('none', dim=dim)
    no_mem_out, _ = no_memory(features, cache=None, return_cache=False)
    print(f"   è¾“å…¥: {features.shape}")
    print(f"   è¾“å‡º: {no_mem_out.shape}")
    print(f"   âœ“ NoMemoryUnit å·¥ä½œæ­£å¸¸")
    
    print("\n" + "="*70)


def test_continual_forecaster():
    """æµ‹è¯•å®Œæ•´çš„ContinualForecaster"""
    print("\n" + "="*70)
    print("æµ‹è¯• ContinualForecaster")
    print("="*70)
    
    batch_size = 4
    seq_len = 32
    input_dim = 3
    output_dim = 1
    pred_len = 1
    
    x = torch.randn(batch_size, seq_len, input_dim)
    
    # æµ‹è¯•ä¸åŒç»„åˆ
    configs = [
        ('lstm', 'titans_mlp', 'add'),
        ('transformer', 'titans_mlp', 'concat'),
        ('titans', 'titans_mlp', 'gated'),
        ('titans', 'none', 'add'),
    ]
    
    for i, (backbone_type, memory_type, fusion_type) in enumerate(configs, 1):
        print(f"\n{i}. æµ‹è¯•ç»„åˆ: {backbone_type} + {memory_type} + {fusion_type}")
        
        model = build_continual_forecaster(
            backbone_type=backbone_type,
            memory_type=memory_type,
            input_dim=input_dim,
            output_dim=output_dim,
            pred_len=pred_len,
            seq_len=seq_len,
            backbone_dim=128,
            backbone_depth=2,
            backbone_heads=4,
            neural_memory_batch_size=256,
            fusion_type=fusion_type
        )
        
        # ä¸ä½¿ç”¨cache
        pred1, _ = model(x, cache=None, return_cache=False)
        print(f"   è¾“å…¥: {x.shape}")
        print(f"   è¾“å‡ºï¼ˆno cacheï¼‰: {pred1.shape}")
        
        # ä½¿ç”¨cache
        pred2, cache = model(x, cache=None, return_cache=True)
        print(f"   è¾“å‡ºï¼ˆwith cacheï¼‰: {pred2.shape}")
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        info = model.get_model_info()
        print(f"   æ¨¡å‹å‚æ•°: {info['total_params']:,}")
        print(f"   âœ“ ç»„åˆ {backbone_type}+{memory_type}+{fusion_type} å·¥ä½œæ­£å¸¸")
    
    print("\n" + "="*70)


def test_gradient_flow():
    """æµ‹è¯•æ¢¯åº¦æµåŠ¨"""
    print("\n" + "="*70)
    print("æµ‹è¯•æ¢¯åº¦æµåŠ¨")
    print("="*70)
    
    batch_size = 4
    seq_len = 32
    input_dim = 3
    output_dim = 1
    pred_len = 1
    
    x = torch.randn(batch_size, seq_len, input_dim)
    y = torch.randn(batch_size, pred_len, output_dim)
    
    # æ„å»ºæ¨¡å‹
    model = build_continual_forecaster(
        backbone_type='titans',
        memory_type='titans_mlp',
        input_dim=input_dim,
        output_dim=output_dim,
        pred_len=pred_len,
        seq_len=seq_len,
        backbone_dim=128,
        backbone_depth=2,
        backbone_heads=4,
        neural_memory_batch_size=256,
        fusion_type='add'
    )
    
    print("\n1. æµ‹è¯•å…¨æ¨¡å‹è®­ç»ƒï¼ˆP+Méƒ½æ›´æ–°ï¼‰...")
    model.train()
    pred, _ = model(x, cache=None, return_cache=False)
    loss = torch.nn.functional.mse_loss(pred, y)
    loss.backward()
    
    # æ£€æŸ¥æ¢¯åº¦
    backbone_has_grad = any(p.grad is not None for p in model.backbone.parameters())
    memory_has_grad = any(p.grad is not None for p in model.memory_unit.parameters())
    
    print(f"   Backboneæœ‰æ¢¯åº¦: {backbone_has_grad}")
    print(f"   Memory Unitæœ‰æ¢¯åº¦: {memory_has_grad}")
    print(f"   âœ“ å…¨æ¨¡å‹è®­ç»ƒæ¢¯åº¦æ­£å¸¸")
    
    # æ¸…é™¤æ¢¯åº¦
    model.zero_grad()
    
    print("\n2. æµ‹è¯•å†»ç»“Backboneï¼ˆä»…Mæ›´æ–°ï¼‰...")
    for param in model.backbone.parameters():
        param.requires_grad = False
    
    pred, _ = model(x, cache=None, return_cache=False)
    loss = torch.nn.functional.mse_loss(pred, y)
    loss.backward()
    
    # æ£€æŸ¥æ¢¯åº¦
    backbone_has_grad = any(p.grad is not None for p in model.backbone.parameters() if p.requires_grad)
    memory_has_grad = any(p.grad is not None for p in model.memory_unit.parameters())
    
    print(f"   Backboneæœ‰æ¢¯åº¦: {backbone_has_grad}")
    print(f"   Memory Unitæœ‰æ¢¯åº¦: {memory_has_grad}")
    print(f"   âœ“ å†»ç»“Backboneæ¢¯åº¦æ­£å¸¸")
    
    print("\n" + "="*70)


def test_cache_mechanism():
    """æµ‹è¯•cacheæœºåˆ¶"""
    print("\n" + "="*70)
    print("æµ‹è¯•Cacheæœºåˆ¶")
    print("="*70)
    
    batch_size = 2
    seq_len = 16
    input_dim = 3
    output_dim = 1
    pred_len = 1
    
    # æ„å»ºæ¨¡å‹
    model = build_continual_forecaster(
        backbone_type='titans',
        memory_type='titans_mlp',
        input_dim=input_dim,
        output_dim=output_dim,
        pred_len=pred_len,
        seq_len=seq_len,
        backbone_dim=64,
        backbone_depth=2,
        backbone_heads=2,
        neural_memory_batch_size=64,
        fusion_type='add'
    )
    
    model.eval()
    
    print("\næ¨¡æ‹Ÿåœ¨çº¿å­¦ä¹ æµç¨‹ï¼ˆ3ä¸ªbatchï¼‰...")
    cache = None
    
    for i in range(3):
        x = torch.randn(batch_size, seq_len, input_dim)
        pred, cache = model(x, cache=cache, return_cache=True)
        
        seq_index = cache[0] if isinstance(cache, tuple) else 0
        print(f"   Batch {i+1}: pred shape={pred.shape}, seq_index={seq_index}")
    
    print(f"   âœ“ Cacheæœºåˆ¶å·¥ä½œæ­£å¸¸ï¼Œseq_indexé€’å¢")
    
    print("\n" + "="*70)


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "ğŸš€ " + "="*66 + " ğŸš€")
    print("  å¼€å§‹æµ‹è¯•æ–°æ¡†æ¶")
    print("ğŸš€ " + "="*66 + " ğŸš€")
    
    try:
        test_backbones()
        test_memory_units()
        test_continual_forecaster()
        test_gradient_flow()
        test_cache_mechanism()
        
        print("\n" + "âœ… " + "="*66 + " âœ…")
        print("  æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ–°æ¡†æ¶å·¥ä½œæ­£å¸¸")
        print("âœ… " + "="*66 + " âœ…\n")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()

