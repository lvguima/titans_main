"""
è®­ç»ƒå™¨æ¨¡å— (v2) - å®ç°å†…å¤–åŒå¾ªç¯è®­ç»ƒé€»è¾‘

è¿™ä¸ªè®­ç»ƒå™¨ä¸models/framework.pyä¸­çš„ContinualForecasteré…åˆä½¿ç”¨
å®ç°äº†è®¾è®¡æ–‡æ¡£ä¸­æè¿°çš„å†…å¤–åŒå¾ªç¯å…ƒå­¦ä¹ æœºåˆ¶ã€‚

æ”¯æŒä¸‰ç§å®éªŒæ¨¡å¼ï¼š
- æ¨¡å¼1 (Baseline): æ ‡å‡†åœ¨çº¿å­¦ä¹ ï¼ˆæ— LMMï¼‰
- æ¨¡å¼2 (Simple TTT): å¸¦LMMï¼Œå›ºå®šæ›´æ–°ç­–ç•¥
- æ¨¡å¼3 (Full Meta-TTT): å¸¦LMMï¼Œå…ƒå­¦ä¹ åŠ¨æ€ç­–ç•¥

è®­ç»ƒæµç¨‹ï¼š
1. **å†…å¾ªç¯ (Inner Loop)**: åœ¨å‰å‘ä¼ æ’­ä¸­ï¼ŒLMMæ ¹æ®æƒŠå¥‡åº¦å®æ—¶æ›´æ–°è‡ªå·±çš„å‚æ•°
2. **å¤–å¾ªç¯ (Outer Loop)**: é€šè¿‡æ ‡å‡†æ¢¯åº¦ä¸‹é™ä¼˜åŒ–Backboneå’ŒMeta-Learner
"""

import os
import time
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import optim
from pathlib import Path
from datetime import datetime

from dataset.data_factory import data_provider, get_data_info
from utils.tools import EarlyStopping, adjust_learning_rate, visual_comprehensive, save_results, get_device
from utils.metrics import metric, print_metrics


class ContinualTrainer:
    """æŒç»­å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self, args):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            args: å‚æ•°é…ç½®å¯¹è±¡
        """
        self.args = args
        self.device = get_device(args)
        
        # è®¾ç½®å®éªŒæ ‡è¯†
        self.setting = f"{args.model_id}_{args.data}_sl{args.seq_len}_pl{args.pred_len}_{args.des}"
        
        # åˆ›å»ºä¿å­˜è·¯å¾„
        self.path = os.path.join(args.checkpoints, self.setting)
        Path(self.path).mkdir(parents=True, exist_ok=True)
        
        # æ„å»ºæ¨¡å‹
        self.model = self._build_model()
        
        # æŸå¤±å‡½æ•°
        self.criterion = self._get_criterion()
        
        # ä¼˜åŒ–å™¨
        self.optimizer = None
    
    def _build_model(self):
        """æ„å»ºæ¨¡å‹"""
        print("\n" + "="*70)
        print("æ„å»ºæŒç»­å­¦ä¹ é¢„æµ‹æ¨¡å‹...")
        print("="*70)
        
        # è‡ªåŠ¨è·å–æ•°æ®ç»´åº¦
        input_dim, output_dim = get_data_info(self.args)
        
        # æ›´æ–°argsä¸­çš„ç»´åº¦ä¿¡æ¯
        self.args.input_dim = input_dim
        self.args.output_dim = output_dim
        
        print(f"\næ•°æ®é…ç½®:")
        print(f"  è¾“å…¥ç»´åº¦: {input_dim}")
        print(f"  è¾“å‡ºç»´åº¦: {output_dim}")
        print(f"  åºåˆ—é•¿åº¦: {self.args.seq_len}")
        print(f"  é¢„æµ‹é•¿åº¦: {self.args.pred_len}")
        
        # ä½¿ç”¨æ–°çš„æ¡†æ¶æ„å»ºæ¨¡å‹
        from models.framework import build_continual_forecaster
        
        model = build_continual_forecaster(
            backbone_type=self.args.backbone_type,
            memory_type=self.args.memory_type,
            input_dim=input_dim,
            output_dim=output_dim,
            pred_len=self.args.pred_len,
            seq_len=self.args.seq_len,
            backbone_dim=self.args.d_model,
            backbone_depth=self.args.e_layers,
            backbone_heads=self.args.n_heads,
            neural_memory_batch_size=self.args.neural_memory_batch_size,
            memory_chunk_size=self.args.memory_chunk_size,
            memory_model_type=self.args.memory_model_type,
            fusion_type=self.args.fusion_type,
            # å…ƒå­¦ä¹ å‚æ•° (æ–°å¢)
            use_meta_learning=getattr(self.args, 'use_meta_learning', 0) == 1,
            meta_learner_type=getattr(self.args, 'meta_learner_type', 'fixed'),
            meta_learner_hidden_dim=getattr(self.args, 'meta_learner_hidden_dim', 128),
        ).to(self.device)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        model_info = model.get_model_info()
        print(f"\næ¨¡å‹æ¶æ„:")
        print(f"  Backbone: {model_info['backbone']}")
        print(f"  Memory Unit: {model_info['memory_unit']}")
        print(f"  ç‰¹å¾ç»´åº¦: {model_info['feature_dim']}")
        print(f"  èåˆæ–¹å¼: {model_info['fusion_type']}")
        print(f"  æ€»å‚æ•°é‡: {model_info['total_params']:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°: {model_info['trainable_params']:,}")
        
        if 'memory_config' in model_info:
            print(f"\nè®°å¿†å•å…ƒé…ç½®:")
            for key, value in model_info['memory_config'].items():
                print(f"  {key}: {value}")
        
        # æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆæ£€æŸ¥æ˜¯å¦æœ‰nanï¼‰
        print("\næµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­...")
        try:
            test_input = torch.randn(1, self.args.seq_len, input_dim).to(self.device)
            with torch.no_grad():
                test_output, _ = model(test_input, cache=None, return_cache=False)
                if torch.isnan(test_output).any() or torch.isinf(test_output).any():
                    print("âš  è­¦å‘Š: æ¨¡å‹åˆå§‹åŒ–åæµ‹è¯•è¾“å‡ºåŒ…å«nan/infï¼")
                    print(f"   æµ‹è¯•è¾“å‡ºèŒƒå›´: [{test_output.min().item():.6f}, {test_output.max().item():.6f}]")
                else:
                    print(f"âœ“ æ¨¡å‹æµ‹è¯•é€šè¿‡ï¼Œè¾“å‡ºèŒƒå›´: [{test_output.min().item():.6f}, {test_output.max().item():.6f}]")
        except Exception as e:
            print(f"âš  è­¦å‘Š: æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        
        print("="*70)
        
        return model
    
    def _get_criterion(self):
        """è·å–æŸå¤±å‡½æ•°"""
        if self.args.loss == 'mse':
            return nn.MSELoss()
        elif self.args.loss == 'mae':
            return nn.L1Loss()
        elif self.args.loss == 'huber':
            return nn.SmoothL1Loss()
        else:
            return nn.MSELoss()
    
    def _get_optimizer(self, params=None):
        """è·å–ä¼˜åŒ–å™¨"""
        if params is None:
            params = self.model.parameters()
        
        if self.args.optimizer == 'adam':
            optimizer = optim.Adam(
                params, 
                lr=self.args.learning_rate,
                weight_decay=self.args.weight_decay
            )
        elif self.args.optimizer == 'adamw':
            optimizer = optim.AdamW(
                params,
                lr=self.args.learning_rate,
                weight_decay=self.args.weight_decay
            )
        elif self.args.optimizer == 'sgd':
            optimizer = optim.SGD(
                params,
                lr=self.args.learning_rate,
                momentum=0.9,
                weight_decay=self.args.weight_decay
            )
        else:
            optimizer = optim.Adam(params, lr=self.args.learning_rate)
        
        return optimizer
    
    def _process_one_batch(self, batch_x, batch_y, cache=None):
        """
        å¤„ç†ä¸€ä¸ªbatch
        
        Args:
            batch_x: è¾“å…¥æ•°æ® [batch_size, seq_len, input_dim]
            batch_y: ç›®æ ‡æ•°æ® [batch_size, label_len + pred_len, output_dim]
            cache: è®°å¿†çŠ¶æ€cache
        
        Returns:
            outputs: é¢„æµ‹ç»“æœ [batch_size, pred_len, output_dim]
            batch_y: çœŸå®æ ‡ç­¾ï¼ˆåªå–pred_lenéƒ¨åˆ†ï¼‰
            next_cache: æ›´æ–°åçš„cacheçŠ¶æ€
        """
        batch_x = batch_x.float().to(self.device)
        batch_y = batch_y.float().to(self.device)
        
        # æ£€æŸ¥è¾“å…¥æ•°æ®æ˜¯å¦æœ‰nan/inf
        if torch.isnan(batch_x).any() or torch.isinf(batch_x).any():
            raise ValueError(f"è¾“å…¥æ•°æ®åŒ…å«nan/infå€¼ï¼")
        if torch.isnan(batch_y).any() or torch.isinf(batch_y).any():
            raise ValueError(f"ç›®æ ‡æ•°æ®åŒ…å«nan/infå€¼ï¼")
        
        # æ ¹æ®æ˜¯å¦ä¼ å…¥cacheå†³å®šæ˜¯å¦ç»´æŠ¤è®°å¿†çŠ¶æ€
        try:
            if cache is not None:
                outputs, next_cache = self.model(batch_x, cache=cache, return_cache=True)
            else:
                outputs, _ = self.model(batch_x, cache=None, return_cache=False)
                next_cache = None
        except RuntimeError as e:
            if "nan" in str(e).lower() or "inf" in str(e).lower():
                raise RuntimeError(f"æ¨¡å‹å‰å‘ä¼ æ’­äº§ç”Ÿnan/inf: {e}")
            raise
        
        # æ£€æŸ¥æ¨¡å‹è¾“å‡ºæ˜¯å¦æœ‰nan/inf
        if outputs is None:
            raise ValueError("æ¨¡å‹è¾“å‡ºä¸ºNoneï¼")
        if torch.isnan(outputs).any() or torch.isinf(outputs).any():
            # æ‰“å°è°ƒè¯•ä¿¡æ¯
            print(f"\nâŒ æ¨¡å‹è¾“å‡ºåŒ…å«nan/infï¼")
            print(f"   è¾“å‡ºå½¢çŠ¶: {outputs.shape}")
            print(f"   nanæ•°é‡: {torch.isnan(outputs).sum().item()}")
            print(f"   infæ•°é‡: {torch.isinf(outputs).sum().item()}")
            print(f"   è¾“å‡ºèŒƒå›´: [{outputs.min().item():.6f}, {outputs.max().item():.6f}]")
            raise ValueError("æ¨¡å‹è¾“å‡ºåŒ…å«nan/infå€¼ï¼")
        
        # æå–é¢„æµ‹éƒ¨åˆ†çš„æ ‡ç­¾
        if self.args.label_len > 0:
            batch_y = batch_y[:, -self.args.pred_len:, :]
        
        return outputs, batch_y, next_cache
    
    def train(self):
        """é¢„è®­ç»ƒé˜¶æ®µï¼šåŒæ—¶è®­ç»ƒPå’ŒM"""
        print("\n" + "="*70)
        print(f"å¼€å§‹é¢„è®­ç»ƒï¼ˆåŒæ—¶è®­ç»ƒBackboneå’ŒMemory Unitï¼‰")
        print(f"å®éªŒè®¾ç½®: {self.setting}")
        print("="*70)
        
        # è·å–æ•°æ®
        train_data, train_loader = data_provider(self.args, flag='train')
        vali_data, vali_loader = data_provider(self.args, flag='val')
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.optimizer = self._get_optimizer()
        
        # æ—©åœ
        early_stopping = EarlyStopping(patience=self.args.patience, verbose=True)
        
        # è®­ç»ƒè®°å½•
        train_steps = len(train_loader)
        
        print(f"\nè®­ç»ƒé…ç½®:")
        print(f"  Epochs: {self.args.train_epochs}")
        print(f"  Batch Size: {self.args.batch_size}")
        print(f"  Learning Rate: {self.args.learning_rate}")
        print(f"  Optimizer: {self.args.optimizer}")
        print(f"  Loss: {self.args.loss}")
        print(f"  Steps per Epoch: {train_steps}")
        
        start_time = time.time()
        
        # è®°å½•è®­ç»ƒå†å²
        train_loss_history = []
        val_loss_history = []
        
        for epoch in range(self.args.train_epochs):
            epoch_time = time.time()
            
            # è®­ç»ƒä¸€ä¸ªepoch
            train_loss = self._train_epoch(train_loader, epoch)
            
            # éªŒè¯
            vali_loss = self._validate(vali_loader)
            
            # è®°å½•losså†å²
            train_loss_history.append(train_loss)
            val_loss_history.append(vali_loss)
            
            # æ‰“å°ä¿¡æ¯
            epoch_duration = time.time() - epoch_time
            print(f"\nEpoch {epoch + 1}/{self.args.train_epochs} | "
                  f"Time: {epoch_duration:.2f}s | "
                  f"Train Loss: {train_loss:.6f} | "
                  f"Val Loss: {vali_loss:.6f}")
            
            # æ—©åœæ£€æŸ¥
            early_stopping(vali_loss, self.model, os.path.join(self.path, 'checkpoint.pth'))
            if early_stopping.early_stop:
                print(f"\næ—©åœè§¦å‘ï¼åœ¨ç¬¬ {epoch + 1} è½®åœæ­¢è®­ç»ƒã€‚")
                break
            
            # å­¦ä¹ ç‡è°ƒæ•´
            if epoch > 0:
                adjust_learning_rate(self.optimizer, epoch + 1, self.args, printout=False)
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        best_model_path = os.path.join(self.path, 'checkpoint.pth')
        self.model.load_state_dict(torch.load(best_model_path))
        
        total_time = time.time() - start_time
        print(f"\né¢„è®­ç»ƒå®Œæˆï¼æ€»ç”¨æ—¶: {total_time:.2f}s")
        
        return self.model
    
    def _train_epoch(self, train_loader, epoch):
        """
        è®­ç»ƒä¸€ä¸ªepoch - å®ç°å¤–å¾ªç¯é€»è¾‘
        
        å¤–å¾ªç¯æ˜¯æ ‡å‡†çš„PyTorchè®­ç»ƒå¾ªç¯ï¼Œä½†åœ¨ä¸€æ¬¡å‰å‘ä¼ æ’­ä¸­
        å·²ç»å®Œæˆäº†æ•´ä¸ªåºåˆ—çš„å†…å¾ªç¯ï¼ˆLMMçš„è®°å¿†æ›´æ–°ï¼‰ã€‚
        
        å…³é”®ç‚¹ï¼š
        1. å‰å‘ä¼ æ’­åŒ…å«äº†å†…å¾ªç¯ï¼ˆLMMè‡ªåŠ¨æ›´æ–°ï¼‰
        2. è®¡ç®—ä»»åŠ¡æŸå¤±ï¼ˆè¯„ä¼°æ•´ä¸ªå†…å¤–å¾ªç¯çš„æ•ˆæœï¼‰
        3. åå‘ä¼ æ’­æ›´æ–°Backboneå’ŒMeta-Learnerï¼ˆæ…¢æƒé‡ï¼‰
        4. LMMçš„å‚æ•°(M)ä¸ç›´æ¥å‚ä¸å¤–å¾ªç¯çš„æ¢¯åº¦ä¸‹é™
        """
        self.model.train()
        train_loss = []
        
        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):
            try:
                # ==== å‰å‘ä¼ æ’­ï¼ˆåŒ…å«å†…å¾ªç¯ï¼‰====
                # è®­ç»ƒæ—¶ä¸ç»´æŠ¤cacheï¼Œæ¯ä¸ªbatchç‹¬ç«‹
                # åœ¨è¿™ä¸ªforwardä¸­ï¼Œæ¨¡å‹å†…éƒ¨ä¼šï¼š
                #   1. å¯¹åºåˆ—çš„æ¯ä¸€æ­¥tï¼š
                #      - Backboneæå–ç‰¹å¾f_t
                #      - Meta-Learnerç”Ÿæˆå…ƒå‚æ•°(Î¸_t, Î·_t, Î±_t)
                #      - LMMæ‰§è¡Œå†…å¾ªç¯æ›´æ–°ï¼ˆå¿«é€Ÿæƒé‡æ›´æ–°ï¼‰
                #      - ä»LMMæ£€ç´¢è®°å¿†
                #   2. èåˆç‰¹å¾å¹¶ç”Ÿæˆæœ€ç»ˆé¢„æµ‹
                outputs, batch_y, _ = self._process_one_batch(batch_x, batch_y)
                
                # ==== è®¡ç®—æœ€ç»ˆä»»åŠ¡æŸå¤± ====
                # è¿™ä¸ªæŸå¤±è¯„ä¼°äº†æ•´ä¸ªå†…å¤–å¾ªç¯è¿‡ç¨‹çš„æœ€ç»ˆæ•ˆæœ
                loss = self.criterion(outputs, batch_y)
                
                # æ£€æŸ¥lossæ˜¯å¦ä¸ºnanæˆ–inf
                if torch.isnan(loss) or torch.isinf(loss):
                    # ç¬¬ä¸€ä¸ªå¼‚å¸¸batchæ‰“å°è¯¦ç»†ä¿¡æ¯
                    if i == 0 or (i + 1) % 100 == 0:
                        print(f"\nâš  è­¦å‘Š: æ£€æµ‹åˆ°æ— æ•ˆæŸå¤±å€¼ (nan/inf) åœ¨ Epoch [{epoch + 1}] Step [{i + 1}]")
                        print(f"   è¾“å‡ºç»Ÿè®¡: min={outputs.min().item():.6f}, max={outputs.max().item():.6f}, mean={outputs.mean().item():.6f}")
                        print(f"   ç›®æ ‡ç»Ÿè®¡: min={batch_y.min().item():.6f}, max={batch_y.max().item():.6f}, mean={batch_y.mean().item():.6f}")
                    continue
                
                train_loss.append(loss.item())
                
                # ==== åå‘ä¼ æ’­ï¼ˆå¤–å¾ªç¯ï¼‰====
                # æ¢¯åº¦ä¼šæµç»ï¼šPrediction Head -> Fusion -> Backbone -> Meta-Learner
                # å…³é”®ï¼šæ¢¯åº¦ä¸ä¼šç›´æ¥æ›´æ–°LMMçš„å‚æ•°Mï¼Œä½†ä¼šæ›´æ–°æŒ‡å¯¼LMMå­¦ä¹ çš„Meta-Learner
                self.optimizer.zero_grad()
                loss.backward()
                
                # æ¢¯åº¦è£å‰ªï¼ˆæ›´ä¸¥æ ¼çš„è£å‰ªï¼Œé˜²æ­¢nanï¼‰
                if self.args.clip_grad > 0:
                    try:
                        grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.clip_grad)
                        # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦å¼‚å¸¸
                        if torch.isnan(grad_norm) or torch.isinf(grad_norm):
                            # å‡å°‘è­¦å‘Šæ‰“å°é¢‘ç‡ï¼ˆæ¯10ä¸ªå¼‚å¸¸batchæ‰“å°ä¸€æ¬¡ï¼‰
                            if (i + 1) % 10 == 0:
                                print(f"\nâš  è­¦å‘Š: æ£€æµ‹åˆ°å¼‚å¸¸æ¢¯åº¦ (nan/inf) åœ¨ Epoch [{epoch + 1}] Step [{i + 1}]")
                            # æ¸…é›¶æ¢¯åº¦å¹¶è·³è¿‡
                            self.optimizer.zero_grad()
                            continue
                    except RuntimeError as e:
                        if "nan" in str(e).lower() or "inf" in str(e).lower():
                            if (i + 1) % 10 == 0:
                                print(f"\nâš  è­¦å‘Š: æ¢¯åº¦è£å‰ªæ—¶å‡ºç°å¼‚å¸¸ åœ¨ Epoch [{epoch + 1}] Step [{i + 1}]")
                            self.optimizer.zero_grad()
                            continue
                        else:
                            raise
                
                # ==== æ›´æ–°"æ…¢æƒé‡"ï¼ˆå¤–å¾ªç¯ï¼‰====
                # æ›´æ–°Backboneå’ŒMeta-Learnerï¼Œè®©å®ƒä»¬åœ¨ä¸‹ä¸€æ¬¡èƒ½æ›´å¥½åœ°æŒ‡å¯¼LMM
                # æ³¨æ„ï¼šLMMçš„æƒé‡Mä¸åœ¨è¿™é‡Œæ›´æ–°ï¼Œå®ƒåœ¨å†…å¾ªç¯ä¸­é€šè¿‡æƒŠå¥‡åº¦æ¢¯åº¦æ›´æ–°
                self.optimizer.step()
                
                # å®šæœŸæ¸…ç†GPUç¼“å­˜ï¼ˆæ¯50ä¸ªbatchï¼Œæ›´é¢‘ç¹ï¼‰
                if (i + 1) % 50 == 0 and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    
                    # å¦‚æœå†…å­˜å ç”¨è¿‡é«˜ï¼Œå¼ºåˆ¶åŒæ­¥
                    allocated = torch.cuda.memory_allocated() / 1e9
                    if allocated > 6.0:  # è¶…è¿‡6GBæ—¶å¼ºåˆ¶åŒæ­¥
                        torch.cuda.synchronize()
                
                # æ‰“å°æ—¥å¿—
                if (i + 1) % self.args.log_interval == 0:
                    if torch.cuda.is_available():
                        allocated = torch.cuda.memory_allocated() / 1e9
                        reserved = torch.cuda.memory_reserved() / 1e9
                        print(f"  Epoch [{epoch + 1}] Step [{i + 1}/{len(train_loader)}] | "
                              f"Loss: {loss.item():.6f} | GPU: {allocated:.2f}/{reserved:.2f} GB")
                    else:
                        print(f"  Epoch [{epoch + 1}] Step [{i + 1}/{len(train_loader)}] | Loss: {loss.item():.6f}")
            
            except RuntimeError as e:
                if "out of memory" in str(e):
                    print(f"\nâš  GPUå†…å­˜ä¸è¶³ï¼å°è¯•æ¸…ç†ç¼“å­˜...")
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    print(f"âš  å»ºè®®å‡å° batch_size æˆ– neural_memory_batch_size")
                    raise
                else:
                    raise
        
        return np.mean(train_loss)
    
    def _validate(self, vali_loader):
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        vali_loss = []
        
        with torch.no_grad():
            for batch_x, batch_y, batch_x_mark, batch_y_mark in vali_loader:
                outputs, batch_y, _ = self._process_one_batch(batch_x, batch_y)
                loss = self.criterion(outputs, batch_y)
                vali_loss.append(loss.item())
        
        return np.mean(vali_loss)
    
    def online_test(self, freeze_backbone=False, load_checkpoint=True):
        """
        åœ¨çº¿æµ‹è¯•é˜¶æ®µ
        
        Args:
            freeze_backbone: True=æ¨¡å¼A(ä»…Må­¦ä¹ ), False=æ¨¡å¼B(Må’ŒPéƒ½å­¦ä¹ )
            load_checkpoint: æ˜¯å¦åŠ è½½é¢„è®­ç»ƒçš„checkpoint
        """
        print("\n" + "="*70)
        mode_name = "æ¨¡å¼A: ä»…è®°å¿†å•å…ƒå­¦ä¹ " if freeze_backbone else "æ¨¡å¼B: å…¨æ¨¡å‹å­¦ä¹ "
        print(f"å¼€å§‹åœ¨çº¿æµ‹è¯•: {mode_name}")
        print(f"å®éªŒè®¾ç½®: {self.setting}")
        print("="*70)
        
        # åŠ è½½checkpoint
        if load_checkpoint:
            checkpoint_path = os.path.join(self.path, 'checkpoint.pth')
            if os.path.exists(checkpoint_path):
                self.model.load_state_dict(torch.load(checkpoint_path))
                print(f"âœ“ å·²åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {checkpoint_path}")
            else:
                print(f"âš  æœªæ‰¾åˆ°checkpoint: {checkpoint_path}")
        
        # è·å–æµ‹è¯•æ•°æ®
        test_data, test_loader = data_provider(self.args, flag='test')
        
        # æ ¹æ®æ¨¡å¼é…ç½®æ¨¡å‹å’Œä¼˜åŒ–å™¨
        if freeze_backbone:
            # æ¨¡å¼A: å†»ç»“Backboneï¼Œåªè®©Memory Unitå­¦ä¹ 
            print("\né…ç½®æ¨¡å¼A:")
            print("  - å†»ç»“Backboneå‚æ•°")
            print("  - Memory Unité€šè¿‡å†…ç½®æœºåˆ¶è‡ªåŠ¨æ›´æ–°")
            print("  - cacheè·¨batchä¼ é€’ï¼Œå®ç°æŒç»­å­¦ä¹ ")
            
            self.model.eval()  # evalæ¨¡å¼ï¼ˆä½†ä¸å½±å“NeuralMemoryçš„å†…éƒ¨æ›´æ–°ï¼‰
            
            # å†»ç»“Backbone
            for param in self.model.backbone.parameters():
                param.requires_grad = False
            
            optimizer = None  # ä¸éœ€è¦å¤–éƒ¨optimizer
            
        else:
            # æ¨¡å¼B: På’ŒMéƒ½å­¦ä¹ 
            print("\né…ç½®æ¨¡å¼B:")
            print(f"  - Backboneå’ŒMemory Unitéƒ½å‚ä¸å­¦ä¹ ")
            print(f"  - ä½¿ç”¨åœ¨çº¿å­¦ä¹ ç‡: {self.args.online_lr}")
            print("  - cacheè·¨batchä¼ é€’ï¼Œå®ç°æŒç»­å­¦ä¹ ")
            
            self.model.train()
            
            # è§£å†»æ‰€æœ‰å‚æ•°
            for param in self.model.parameters():
                param.requires_grad = True
            
            optimizer = optim.Adam(
                filter(lambda p: p.requires_grad, self.model.parameters()),
                lr=self.args.online_lr
            )
        
        print("="*70 + "\n")
        
        # åœ¨çº¿æµ‹è¯•å¾ªç¯
        preds = []
        trues = []
        losses = []
        
        # ğŸ”‘ å…³é”®ï¼šåˆå§‹åŒ–cacheä»¥ç»´æŠ¤è®°å¿†çŠ¶æ€
        cache = None
        
        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):
            # ğŸ”‘ å…³é”®ï¼šä¼ å…¥cacheç»´æŒè®°å¿†çŠ¶æ€
            outputs, batch_y, cache = self._process_one_batch(batch_x, batch_y, cache)
            
            # è®¡ç®—æŸå¤±
            loss = self.criterion(outputs, batch_y)
            loss_per_sample = F.mse_loss(outputs, batch_y, reduction='none').mean(dim=(1,2))
            losses.extend(loss_per_sample.detach().cpu().numpy().tolist())
            
            # åœ¨çº¿æ›´æ–°ï¼ˆä»…æ¨¡å¼Béœ€è¦ï¼‰
            if optimizer is not None:
                optimizer.zero_grad()
                loss.backward()
                
                if self.args.clip_grad > 0:
                    torch.nn.utils.clip_grad_norm_(
                        filter(lambda p: p.requires_grad, self.model.parameters()), 
                        self.args.clip_grad
                    )
                
                optimizer.step()
            
            # æ”¶é›†é¢„æµ‹å’ŒçœŸå®å€¼
            pred = outputs.detach().cpu().numpy()
            true = batch_y.detach().cpu().numpy()
            
            preds.append(pred)
            trues.append(true)
            
            # æ‰“å°è¿›åº¦
            if (i + 1) % 10 == 0:
                recent_loss = np.mean(losses[-320:]) if len(losses) >= 320 else np.mean(losses)
                print(f"  è¿›åº¦: Batch [{i+1}/{len(test_loader)}] | Recent Loss: {recent_loss:.6f}")
        
        return self._finalize_test_results(preds, trues, losses, test_data, freeze_backbone)
    
    def _finalize_test_results(self, preds, trues, losses, test_data, freeze_backbone):
        """æ•´ç†æµ‹è¯•ç»“æœå¹¶è¿›è¡Œè¯„ä¼°ã€å¯è§†åŒ–"""
        
        # åˆå¹¶ç»“æœ
        preds = np.concatenate(preds, axis=0)
        trues = np.concatenate(trues, axis=0)
        
        print(f"\né¢„æµ‹å½¢çŠ¶: {preds.shape}")
        print(f"çœŸå®å€¼å½¢çŠ¶: {trues.shape}")
        
        # åæ ‡å‡†åŒ–
        if hasattr(test_data, 'inverse_transform'):
            preds_orig = test_data.inverse_transform(preds.reshape(-1, preds.shape[-1]))
            trues_orig = test_data.inverse_transform(trues.reshape(-1, trues.shape[-1]))
            preds_orig = preds_orig.reshape(preds.shape)
            trues_orig = trues_orig.reshape(trues.shape)
        else:
            preds_orig = preds
            trues_orig = trues
        
        # è®¡ç®—æŒ‡æ ‡
        mae, mse, rmse, mape, mspe, rse, corr = metric(preds_orig, trues_orig)
        print_metrics(mae, mse, rmse, mape, mspe, rse, corr)
        
        # ä¿å­˜ç»“æœ
        if self.args.save_pred:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            mode_suffix = 'memory_only' if freeze_backbone else 'full_model'
            result_path = os.path.join(self.args.result_path, f'{self.setting}_online_{mode_suffix}_{timestamp}.csv')
            save_results(trues_orig, preds_orig, losses, result_path)
            print(f"âœ“ ç»“æœå·²ä¿å­˜åˆ°: {result_path}")
        
        # å¯è§†åŒ–
        if self.args.save_fig:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            mode_suffix = 'memory_only' if freeze_backbone else 'full_model'
            fig_path = os.path.join(self.args.fig_path, f'{self.setting}_online_{mode_suffix}_{timestamp}.jpg')
            visual_comprehensive(
                trues_orig.flatten(), 
                preds_orig.flatten(), 
                losses if len(losses) > 0 else None,
                fig_path,
                train_size=None
            )
            print(f"âœ“ å¯è§†åŒ–å·²ä¿å­˜åˆ°: {fig_path}")
        
        return mae, mse, rmse, mape, mspe, rse


if __name__ == '__main__':
    print("è®­ç»ƒå™¨æ¨¡å—å·²åˆ›å»º")
    print("è¯·é€šè¿‡titans_main.pyè¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹")

