"""
è®­ç»ƒå™¨æ¨¡å—
å¤„ç†æ¨¡å‹çš„è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•
"""

import os
import time
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import optim
from pathlib import Path
from datetime import datetime

from models.titans_mac import build_model
from dataset.data_factory import data_provider
from utils.tools import EarlyStopping, adjust_learning_rate, visual, visual_loss_curve, visual_comprehensive, save_results, create_experiment_folder, get_device
from utils.metrics import metric, print_metrics


class Trainer:
    """Titansæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, args):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            args: å‚æ•°é…ç½®å¯¹è±¡
        """
        self.args = args
        self.device = get_device(args)
        
        # è®¾ç½®å®éªŒæ ‡è¯†
        self.model_id = args.model_id
        self.setting = f"{args.model_id}_{args.data}_sl{args.seq_len}_pl{args.pred_len}_{args.des}"
        
        # åˆ›å»ºä¿å­˜è·¯å¾„
        self.path = os.path.join(args.checkpoints, self.setting)
        Path(self.path).mkdir(parents=True, exist_ok=True)
        
        # æ„å»ºæ¨¡å‹
        self.model = self._build_model()
        
        # æŸå¤±å‡½æ•°
        self.criterion = self._get_criterion()
        
        # ä¼˜åŒ–å™¨
        self.optimizer = None
    
    def _build_model(self):
        """æ„å»ºæ¨¡å‹"""
        print("\n" + "="*70)
        print("æ„å»ºTitans MACæ¨¡å‹...")
        print("="*70)
        
        # è‡ªåŠ¨è·å–æ•°æ®ç»´åº¦
        from dataset.data_factory import get_data_info
        input_dim, output_dim = get_data_info(self.args)
        
        # æ›´æ–°argsä¸­çš„ç»´åº¦ä¿¡æ¯
        self.args.input_dim = input_dim
        self.args.output_dim = output_dim
        
        # æ„å»ºæ¨¡å‹
        model = build_model(self.args).to(self.device)
        
        return model
    
    def _get_criterion(self):
        """è·å–æŸå¤±å‡½æ•°"""
        if self.args.loss == 'mse':
            return nn.MSELoss()
        elif self.args.loss == 'mae':
            return nn.L1Loss()
        elif self.args.loss == 'huber':
            return nn.SmoothL1Loss()
        else:
            return nn.MSELoss()
    
    def _get_optimizer(self):
        """è·å–ä¼˜åŒ–å™¨"""
        if self.args.optimizer == 'adam':
            optimizer = optim.Adam(
                self.model.parameters(), 
                lr=self.args.learning_rate,
                weight_decay=self.args.weight_decay
            )
        elif self.args.optimizer == 'adamw':
            optimizer = optim.AdamW(
                self.model.parameters(),
                lr=self.args.learning_rate,
                weight_decay=self.args.weight_decay
            )
        elif self.args.optimizer == 'sgd':
            optimizer = optim.SGD(
                self.model.parameters(),
                lr=self.args.learning_rate,
                momentum=0.9,
                weight_decay=self.args.weight_decay
            )
        else:
            optimizer = optim.Adam(self.model.parameters(), lr=self.args.learning_rate)
        
        return optimizer
    
    def _process_one_batch(self, batch_x, batch_y, neural_mem_state=None):
        """
        å¤„ç†ä¸€ä¸ªbatch
        
        Args:
            batch_x: è¾“å…¥æ•°æ® [batch_size, seq_len, input_dim]
            batch_y: ç›®æ ‡æ•°æ® [batch_size, label_len + pred_len, output_dim]
            neural_mem_state: NeuralMemoryçš„cacheçŠ¶æ€ (seq_index, kv_caches, neural_mem_caches)
        
        Returns:
            outputs: é¢„æµ‹ç»“æœ [batch_size, pred_len, output_dim]
            batch_y: çœŸå®æ ‡ç­¾ï¼ˆåªå–pred_lenéƒ¨åˆ†ï¼‰
            next_neural_mem_state: æ›´æ–°åçš„cacheçŠ¶æ€ï¼ˆå¦‚æœä¼ å…¥äº†cacheï¼‰
        """
        batch_x = batch_x.float().to(self.device)
        batch_y = batch_y.float().to(self.device)
        
        # ğŸ”‘ å…³é”®ï¼šcacheæœºåˆ¶æ§åˆ¶è®°å¿†çš„ç´¯ç§¯å­¦ä¹ 
        if neural_mem_state is not None:
            # åœ¨çº¿å­¦ä¹ æ¨¡å¼ï¼šä¼ å…¥cacheå¹¶è·å–æ›´æ–°åçš„cache
            # cacheç»´æŠ¤seq_indexã€kv_cacheså’Œneural_mem_caches
            # è¿™æ ·NeuralMemoryå¯ä»¥è·¨batchç´¯ç§¯å­¦ä¹ 
            outputs, next_neural_mem_state = self.model(
                batch_x, 
                cache=neural_mem_state, 
                return_cache=True
            )
            
            # å¤„ç†longterm_mem tokençš„ç‰¹æ®Šæƒ…å†µ
            # åŸå§‹åº“åœ¨æŸäº›ä½ç½®ä¼šè¿”å›Noneï¼ˆè·³è¿‡longterm_mem tokensï¼‰
            if outputs is None:
                # è¿”å›ç©ºé¢„æµ‹ï¼Œä½†ä¿ç•™cacheä¾›ä¸‹ä¸€ä¸ªbatchä½¿ç”¨
                return None, batch_y, next_neural_mem_state
        else:
            # è®­ç»ƒæ¨¡å¼æˆ–æ— è®°å¿†ç´¯ç§¯æ¨¡å¼ï¼šä¸ç»´æŠ¤cache
            # æ¯ä¸ªbatchç‹¬ç«‹å¤„ç†
            outputs = self.model(batch_x)
            next_neural_mem_state = None
        
        # æå–é¢„æµ‹éƒ¨åˆ†çš„æ ‡ç­¾
        if self.args.label_len > 0:
            batch_y = batch_y[:, -self.args.pred_len:, :]
        
        # è°ƒæ•´è¾“å‡ºå½¢çŠ¶ä»¥åŒ¹é…æ ‡ç­¾
        if outputs.dim() == 2:
            outputs = outputs.unsqueeze(1)  # [B, 1, D]
        
        return outputs, batch_y, next_neural_mem_state
    
    def train(self):
        """è®­ç»ƒæ¨¡å‹"""
        print("\n" + "="*70)
        print(f"å¼€å§‹è®­ç»ƒ: {self.setting}")
        print("="*70)
        
        # è·å–æ•°æ®
        train_data, train_loader = data_provider(self.args, flag='train')
        vali_data, vali_loader = data_provider(self.args, flag='val')
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.optimizer = self._get_optimizer()
        
        # æ—©åœ
        early_stopping = EarlyStopping(patience=self.args.patience, verbose=True)
        
        # è®­ç»ƒè®°å½•
        train_steps = len(train_loader)
        
        print(f"\nè®­ç»ƒé…ç½®:")
        print(f"  Epochs: {self.args.train_epochs}")
        print(f"  Batch Size: {self.args.batch_size}")
        print(f"  Learning Rate: {self.args.learning_rate}")
        print(f"  Optimizer: {self.args.optimizer}")
        print(f"  Loss: {self.args.loss}")
        print(f"  Steps per Epoch: {train_steps}")
        
        start_time = time.time()
        
        # è®°å½•è®­ç»ƒå†å²
        train_loss_history = []
        val_loss_history = []
        
        for epoch in range(self.args.train_epochs):
            epoch_time = time.time()
            
            # è®­ç»ƒä¸€ä¸ªepoch
            train_loss = self._train_epoch(train_loader, epoch)
            
            # éªŒè¯
            vali_loss = self._validate(vali_loader)
            
            # è®°å½•losså†å²
            train_loss_history.append(train_loss)
            val_loss_history.append(vali_loss)
            
            # æ‰“å°ä¿¡æ¯
            epoch_duration = time.time() - epoch_time
            print(f"\nEpoch {epoch + 1}/{self.args.train_epochs} | "
                  f"Time: {epoch_duration:.2f}s | "
                  f"Train Loss: {train_loss:.6f} | "
                  f"Val Loss: {vali_loss:.6f}")
            
            # æ—©åœæ£€æŸ¥
            early_stopping(vali_loss, self.model, os.path.join(self.path, 'checkpoint.pth'))
            if early_stopping.early_stop:
                print(f"\næ—©åœè§¦å‘ï¼åœ¨ç¬¬ {epoch + 1} è½®åœæ­¢è®­ç»ƒã€‚")
                break
            
            # å­¦ä¹ ç‡è°ƒæ•´
            if epoch > 0:
                adjust_learning_rate(self.optimizer, epoch + 1, self.args, printout=False)
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        best_model_path = os.path.join(self.path, 'checkpoint.pth')
        self.model.load_state_dict(torch.load(best_model_path))
        
        total_time = time.time() - start_time
        print(f"\nè®­ç»ƒå®Œæˆï¼æ€»ç”¨æ—¶: {total_time:.2f}s")
        
        # ä¿å­˜è®­ç»ƒlossæ›²çº¿ï¼ˆå·²ç¦ç”¨ï¼Œç”¨æˆ·ä¸éœ€è¦ï¼‰
        # if self.args.save_fig and len(train_loss_history) > 0:
        #     timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        #     loss_curve_path = os.path.join(self.args.fig_path, f'{self.setting}_loss_curve_{timestamp}.jpg')
        #     visual_loss_curve(train_loss_history, val_loss_history, loss_curve_path)
        #     print(f"âœ“ è®­ç»ƒlossæ›²çº¿å·²ä¿å­˜åˆ°: {loss_curve_path}")
        
        return self.model
    
    def _train_epoch(self, train_loader, epoch):
        """è®­ç»ƒä¸€ä¸ªepochï¼ˆæ”¯æŒæ¢¯åº¦ç´¯ç§¯å’Œç¨€ç–æ ‡ç­¾ï¼‰"""
        self.model.train()
        train_loss = []
        
        # æ¢¯åº¦ç´¯ç§¯ï¼šåªåœ¨ç´¯ç§¯åˆ°æŒ‡å®šæ­¥æ•°æ—¶æ‰æ›´æ–°å‚æ•°
        update_freq = self.args.train_update_freq
        
        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):
            # å‰å‘ä¼ æ’­
            outputs, batch_y, _ = self._process_one_batch(batch_x, batch_y)
            
            # è®­ç»ƒé˜¶æ®µï¼šæ€»æ˜¯ä½¿ç”¨æ‰€æœ‰æ ‡ç­¾ï¼ˆç¨€ç–æ ‡ç­¾åªå½±å“æµ‹è¯•é˜¶æ®µï¼‰
            # è®¡ç®—æŸå¤±ï¼ˆéœ€è¦é™¤ä»¥ç´¯ç§¯æ­¥æ•°ï¼Œä»¥ä¿æŒæ¢¯åº¦å°ºåº¦ä¸€è‡´ï¼‰
            loss = self.criterion(outputs, batch_y) / update_freq
            train_loss.append(loss.item() * update_freq)  # è®°å½•åŸå§‹æŸå¤±å€¼
            
            # åå‘ä¼ æ’­ï¼ˆç´¯ç§¯æ¢¯åº¦ï¼‰
            loss.backward()
            
            # æ¯éš” update_freq æ­¥æ›´æ–°ä¸€æ¬¡å‚æ•°
            if (i + 1) % update_freq == 0 or (i + 1) == len(train_loader):
                # æ¢¯åº¦è£å‰ª
                if self.args.clip_grad > 0:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.clip_grad)
                
                # æ›´æ–°å‚æ•°
                self.optimizer.step()
                self.optimizer.zero_grad()
            
            # æ‰“å°æ—¥å¿—
            if (i + 1) % self.args.log_interval == 0:
                print(f"  Epoch [{epoch + 1}] Step [{i + 1}/{len(train_loader)}] | Loss: {loss.item() * update_freq:.6f}")
        
        return np.mean(train_loss)
    
    def _validate(self, vali_loader):
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        vali_loss = []
        
        with torch.no_grad():
            for batch_x, batch_y, batch_x_mark, batch_y_mark in vali_loader:
                outputs, batch_y, _ = self._process_one_batch(batch_x, batch_y)
                loss = self.criterion(outputs, batch_y)
                vali_loss.append(loss.item())
        
        return np.mean(vali_loss)
    
    def test(self, load_checkpoint=True):
        """æµ‹è¯•æ¨¡å‹ï¼ˆä¸‰ç§æ¨¡å¼ï¼šé™æ€/åœ¨çº¿è®°å¿†/åœ¨çº¿å…¨æ¨¡å‹ï¼‰"""
        print("\n" + "="*70)
        print(f"å¼€å§‹æµ‹è¯•: {self.setting}")
        print("="*70)
        
        # åŠ è½½checkpoint
        if load_checkpoint:
            checkpoint_path = os.path.join(self.path, 'checkpoint.pth')
            if os.path.exists(checkpoint_path):
                self.model.load_state_dict(torch.load(checkpoint_path))
                print(f"âœ“ å·²åŠ è½½æ¨¡å‹: {checkpoint_path}")
        
        # è·å–æµ‹è¯•æ•°æ®
        test_data, test_loader = data_provider(self.args, flag='test')
        
        # æ ¹æ®æ˜¯å¦å¯ç”¨åœ¨çº¿å­¦ä¹ é€‰æ‹©æµ‹è¯•æ–¹å¼
        if self.args.online_learning:
            return self._test_with_online_learning(test_data, test_loader)
        else:
            return self._test_no_memory_accumulation(test_data, test_loader)
    
    def _test_no_memory_accumulation(self, test_data, test_loader):
        """
        æ¨¡å¼Aï¼šæ— è®°å¿†ç´¯ç§¯æ¨¡å¼
        
        è¡Œä¸ºï¼š
        - æ¯ä¸ªbatchç‹¬ç«‹å¤„ç†ï¼Œä¸ç»´æŠ¤cache
        - NeuralMemoryåœ¨batchå†…è‡ªåŠ¨æ›´æ–°ï¼ˆè¿™æ˜¯åŸå§‹åº“çš„å›ºæœ‰æœºåˆ¶ï¼Œæ— æ³•å…³é—­ï¼‰
        - batchä¹‹é—´ä¸ç´¯ç§¯è®°å¿†çŠ¶æ€
        
        ç›¸å½“äºï¼šçŸ­æœŸè®°å¿†æ¨¡å¼ï¼Œæµ‹è¯•é¢„è®­ç»ƒæ¨¡å‹çš„å³æ—¶æ³›åŒ–èƒ½åŠ›
        """
        print("=" * 70)
        print("æµ‹è¯•æ¨¡å¼A: æ— è®°å¿†ç´¯ç§¯ï¼ˆæ¯ä¸ªbatchç‹¬ç«‹å¤„ç†ï¼‰")
        print("  - NeuralMemoryåœ¨batchå†…è‡ªåŠ¨æ›´æ–°ï¼ˆåŸå§‹åº“å›ºæœ‰æœºåˆ¶ï¼‰")
        print("  - batchä¹‹é—´ä¸ä¼ é€’cacheï¼Œè®°å¿†çŠ¶æ€ä¸ç´¯ç§¯")
        print("  - ç›¸å½“äº'çŸ­æœŸè®°å¿†'æ¨¡å¼")
        print("=" * 70 + "\n")
        
        self.model.eval()
        
        preds = []
        trues = []
        losses = []
        
        # ğŸ”‘ å…³é”®ä¿®æ”¹ï¼šä¸ä½¿ç”¨torch.no_grad()ï¼Œè®©NeuralMemoryå¯ä»¥æ­£å¸¸è®¡ç®—surprise
        # torch.no_grad()ä¼šç¦ç”¨NeuralMemoryå†…éƒ¨çš„torch.func.gradè®¡ç®—
        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):
            # ğŸ”‘ å…³é”®ï¼šä¸ä¼ cacheï¼Œæ¯ä¸ªbatchéƒ½æ˜¯ç‹¬ç«‹çš„
            outputs, batch_y, _ = self._process_one_batch(batch_x, batch_y, neural_mem_state=None)
            
            # è®°å½•æ¯ä¸ªæ ·æœ¬çš„loss
            with torch.no_grad():  # åªåœ¨è®¡ç®—lossæ—¶ä½¿ç”¨no_grad
                loss_per_sample = F.mse_loss(outputs, batch_y, reduction='none').mean(dim=(1,2))
                losses.extend(loss_per_sample.detach().cpu().numpy().tolist())
            
            # æ”¶é›†é¢„æµ‹å’ŒçœŸå®å€¼
            pred = outputs.detach().cpu().numpy()
            true = batch_y.detach().cpu().numpy()
            
            preds.append(pred)
            trues.append(true)
            
            # æ‰“å°è¿›åº¦
            if (i + 1) % 10 == 0:  # æ”¹ä¸ºæ¯10ä¸ªbatchæ‰“å°ä¸€æ¬¡
                recent_loss = np.mean(losses[-320:]) if len(losses) >= 320 else np.mean(losses)
                print(f"  è¿›åº¦: Batch [{i+1}/{len(test_loader)}] | Recent Loss: {recent_loss:.6f}")
        
        return self._finalize_test_results(preds, trues, losses, test_data)
    
    def _test_with_online_learning(self, test_data, test_loader):
        """
        åœ¨çº¿å­¦ä¹ æµ‹è¯•ï¼ˆTitansæŒç»­å­¦ä¹ æ¨¡å¼ï¼‰
        
        æ ¸å¿ƒæœºåˆ¶ï¼š
        - NeuralMemoryåœ¨forwardæ—¶è‡ªåŠ¨å®Œæˆ store + retrieve
        - Store: è®¡ç®—grad(MSE(M(k), v))ï¼Œä½¿ç”¨è‡ªé€‚åº”lr/åŠ¨é‡/é—å¿˜æ›´æ–°è®°å¿†æƒé‡
        - Retrieve: è¿”å›M(q)ä½œä¸ºcontext
        - ğŸ”‘ cacheè·¨batchä¼ é€’ï¼Œå®ç°è®°å¿†çš„ç´¯ç§¯å­¦ä¹ 
        
        ä¸¤ç§å­æ¨¡å¼ï¼š
        æ¨¡å¼B: online_update_memory_only=True
          - åªè®©NeuralMemoryè‡ªåŠ¨æ›´æ–°ï¼Œbackboneå†»ç»“
          - è½»é‡çº§é€‚åº”ï¼Œé¿å…ç¾éš¾æ€§é—å¿˜
        
        æ¨¡å¼C: online_update_memory_only=False
          - NeuralMemoryè‡ªåŠ¨æ›´æ–° + åå‘ä¼ æ’­æ›´æ–°backbone
          - æœ€å¤§é€‚åº”èƒ½åŠ›ï¼Œä½†å¯èƒ½è¿‡æ‹Ÿåˆ
        """
        mode_name = "æ¨¡å¼B: åœ¨çº¿å­¦ä¹  - ä»…è®°å¿†æ›´æ–°" if self.args.online_update_memory_only else "æ¨¡å¼C: åœ¨çº¿å­¦ä¹  - å…¨æ¨¡å‹æ›´æ–°"
        
        print("=" * 70)
        print(mode_name)
        print(f"  - æ›´æ–°ç­–ç•¥: {'ä»…NeuralMemoryè‡ªé€‚åº”' if self.args.online_update_memory_only else 'NeuralMemory + BackboneåŒæ—¶æ›´æ–°'}")
        
        if self.args.online_update_memory_only:
            # === æ¨¡å¼B: ä¿¡ä»»NeuralMemoryè‡ªåŠ¨æ›´æ–° ===
            print("  - NeuralMemoryåœ¨forwardæ—¶è‡ªåŠ¨æ›´æ–°ï¼ˆè‡ªé€‚åº”lrã€åŠ¨é‡ã€é—å¿˜ï¼‰")
            print("  - Backboneå®Œå…¨å†»ç»“")
            print("  - cacheè·¨batchä¼ é€’ï¼Œè®°å¿†çŠ¶æ€ç´¯ç§¯å­¦ä¹ ")
            print("=" * 70 + "\n")
            
            self.model.eval()  # å†»ç»“BN/Dropout
            if hasattr(self.model, 'freeze_non_memory_params'):
                self.model.freeze_non_memory_params()
            
            # ä¸åˆ›å»ºoptimizerï¼å®Œå…¨ä¿¡ä»»NeuralMemoryçš„è‡ªåŒ…å«æ›´æ–°
            online_optimizer = None
        else:
            # === æ¨¡å¼C: NeuralMemoryè‡ªåŠ¨æ›´æ–° + å¤–éƒ¨optimizeræ›´æ–°backbone ===
            print(f"  - NeuralMemoryè‡ªåŠ¨æ›´æ–° + Backboneé€šè¿‡åå‘ä¼ æ’­æ›´æ–°ï¼ˆlr={self.args.online_lr}ï¼‰")
            print("  - cacheè·¨batchä¼ é€’ï¼Œè®°å¿†çŠ¶æ€ç´¯ç§¯å­¦ä¹ ")
            print("=" * 70 + "\n")
            
            self.model.train()
            online_optimizer = torch.optim.Adam(
                filter(lambda p: p.requires_grad, self.model.parameters()),
                lr=self.args.online_lr
            )
        
        preds = []
        trues = []
        losses = []
        
        # ğŸ”‘ å…³é”®ï¼šåˆå§‹åŒ–NeuralMemory cacheä»¥ç»´æŠ¤åœ¨çº¿å­¦ä¹ çŠ¶æ€
        # cacheæ ¼å¼: (seq_index, kv_caches, neural_mem_caches)
        # é¦–æ¬¡è°ƒç”¨æ—¶ä¼ å…¥Noneï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨åˆå§‹åŒ–
        # åç»­æ¯æ¬¡forwardä¼šè¿”å›æ›´æ–°åçš„cacheï¼ŒæŒç»­ä¼ å…¥ä»¥å®ç°è®°å¿†çš„ç´¯ç§¯å­¦ä¹ 
        neural_mem_state = None
        
        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):
            # ç¨€ç–æ ‡ç­¾ï¼šæ§åˆ¶ä½•æ—¶ä½¿ç”¨çœŸå®æ ‡ç­¾
            use_label = True
            if self.args.sparse_label:
                use_label = (i % self.args.sparse_step == 0)
            
            if use_label:
                # === æœ‰æ ‡ç­¾ï¼šNeuralMemoryå¯ä»¥å­¦ä¹  ===
                # ğŸ”‘ å…³é”®ï¼šä¼ å…¥neural_mem_stateç»´æŒè®°å¿†çŠ¶æ€ï¼
                outputs, batch_y, neural_mem_state = self._process_one_batch(batch_x, batch_y, neural_mem_state)
                
                # å¤„ç†longterm_mem tokençš„ç‰¹æ®Šæƒ…å†µï¼ˆoutputså¯èƒ½ä¸ºNoneï¼‰
                if outputs is None:
                    continue
                
                loss = self.criterion(outputs, batch_y)
                # è®°å½•æ¯ä¸ªæ ·æœ¬çš„lossï¼ˆreduction='none'ç„¶åflattenï¼‰
                loss_per_sample = F.mse_loss(outputs, batch_y, reduction='none').mean(dim=(1,2))
                losses.extend(loss_per_sample.detach().cpu().numpy().tolist())
                
                # åªåœ¨æ¨¡å¼Cæ—¶æ‰åå‘ä¼ æ’­æ›´æ–°backbone
                if online_optimizer is not None:
                    online_optimizer.zero_grad()
                    loss.backward()
                    
                    if self.args.clip_grad > 0:
                        torch.nn.utils.clip_grad_norm_(
                            filter(lambda p: p.requires_grad, self.model.parameters()), 
                            self.args.clip_grad
                        )
                    
                    online_optimizer.step()
                # æ¨¡å¼Bæ—¶ï¼šNeuralMemoryå·²åœ¨forwardä¸­è‡ªåŠ¨æ›´æ–°ï¼Œstateå·²ä¿å­˜åœ¨cacheä¸­
            else:
                # === æ— æ ‡ç­¾ï¼šåªé¢„æµ‹ï¼Œä¸æ›´æ–°ï¼ˆæ¨¡æ‹Ÿç¨€ç–æ ‡ç­¾ï¼‰ ===
                with torch.no_grad():
                    outputs, batch_y, neural_mem_state = self._process_one_batch(batch_x, batch_y, neural_mem_state)
                    
                    # å¤„ç†longterm_mem tokençš„ç‰¹æ®Šæƒ…å†µ
                    if outputs is None:
                        continue
                    
                    # è®°å½•æ¯ä¸ªæ ·æœ¬çš„loss
                    loss_per_sample = F.mse_loss(outputs, batch_y, reduction='none').mean(dim=(1,2))
                    losses.extend(loss_per_sample.detach().cpu().numpy().tolist())
            
            # ä¿å­˜é¢„æµ‹ç»“æœ
            pred = outputs.detach().cpu().numpy()
            true = batch_y.detach().cpu().numpy()
            
            preds.append(pred)
            trues.append(true)
            
            # æ‰“å°è¿›åº¦
            if (i + 1) % 10 == 0:  # æ”¹ä¸ºæ¯10ä¸ªbatchæ‰“å°ä¸€æ¬¡ï¼ˆå› ä¸ºç°åœ¨batch_size=32ï¼‰
                label_marker = " [ç¨€ç–-æ— æ ‡ç­¾]" if self.args.sparse_label and not use_label else ""
                recent_loss = np.mean(losses[-320:]) if len(losses) >= 320 else np.mean(losses)  # æœ€è¿‘320ä¸ªæ ·æœ¬(çº¦10ä¸ªbatch)
                print(f"  è¿›åº¦: Batch [{i+1}/{len(test_loader)}] | Recent Loss: {recent_loss:.6f}{label_marker}")
        
        return self._finalize_test_results(preds, trues, losses, test_data)
    
    def _finalize_test_results(self, preds, trues, losses, test_data):
        """æ•´ç†æµ‹è¯•ç»“æœå¹¶è¿›è¡Œè¯„ä¼°ã€å¯è§†åŒ–"""
        
        # åˆå¹¶ç»“æœ
        preds = np.concatenate(preds, axis=0)
        trues = np.concatenate(trues, axis=0)
        
        print(f"\né¢„æµ‹å½¢çŠ¶: {preds.shape}")
        print(f"çœŸå®å€¼å½¢çŠ¶: {trues.shape}")
        
        # åæ ‡å‡†åŒ–
        if hasattr(test_data, 'inverse_transform'):
            preds_orig = test_data.inverse_transform(preds.reshape(-1, preds.shape[-1]))
            trues_orig = test_data.inverse_transform(trues.reshape(-1, trues.shape[-1]))
            preds_orig = preds_orig.reshape(preds.shape)
            trues_orig = trues_orig.reshape(trues.shape)
        else:
            preds_orig = preds
            trues_orig = trues
        
        # è®¡ç®—æŒ‡æ ‡
        mae, mse, rmse, mape, mspe, rse, corr = metric(preds_orig, trues_orig)
        print_metrics(mae, mse, rmse, mape, mspe, rse, corr)
        
        # ä¿å­˜ç»“æœ
        if self.args.save_pred:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            mode_suffix = 'online' if self.args.online_learning else 'static'
            result_path = os.path.join(self.args.result_path, f'{self.setting}_test_{mode_suffix}_{timestamp}.csv')
            save_results(trues_orig, preds_orig, losses, result_path)
        
        # å¯è§†åŒ–ï¼ˆJPGæ ¼å¼ï¼‰
        if self.args.save_fig:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            mode_suffix = 'online' if self.args.online_learning else 'static'
            
            # ç»¼åˆå¯è§†åŒ–ï¼ˆç±»ä¼¼experiment_comparison.pyçš„é£æ ¼ï¼‰
            comprehensive_fig_path = os.path.join(self.args.fig_path, f'{self.setting}_test_{mode_suffix}_{timestamp}.jpg')
            train_size = None  # å¦‚æœæ˜¯å•ç‹¬æµ‹è¯•ï¼Œæ— æ³•å¾—çŸ¥è®­ç»ƒé›†å¤§å°
            visual_comprehensive(
                trues_orig.flatten(), 
                preds_orig.flatten(), 
                losses if len(losses) > 0 else None,
                comprehensive_fig_path,
                train_size=train_size
            )
            print(f"âœ“ æµ‹è¯•ç»“æœå¯è§†åŒ–å·²ä¿å­˜åˆ°: {comprehensive_fig_path}")
        
        return mae, mse, rmse, mape, mspe, rse


if __name__ == '__main__':
    """æµ‹è¯•è®­ç»ƒå™¨"""
    print("è®­ç»ƒå™¨æ¨¡å—å·²åˆ›å»º")
    print("è¯·é€šè¿‡titans_main.pyè¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹")

