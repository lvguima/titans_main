"""
æ–°çš„è®­ç»ƒå™¨æ¨¡å— - é€‚é…æ¨¡å—åŒ–æ¡†æ¶

è¿™ä¸ªè®­ç»ƒå™¨ä¸models/framework.pyä¸­çš„ContinualForecasteré…åˆä½¿ç”¨
æ”¯æŒï¼š
- æ¨¡å—åŒ–çš„Backboneå’ŒMemory Unit
- é¢„è®­ç»ƒé˜¶æ®µï¼šåŒæ—¶è®­ç»ƒPå’ŒM
- åœ¨çº¿æµ‹è¯•é˜¶æ®µï¼š
    - æ¨¡å¼A: ä»…Må­¦ä¹ ï¼ˆPå†»ç»“ï¼‰
    - æ¨¡å¼B: Må’ŒPéƒ½å­¦ä¹ 
"""

import os
import time
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import optim
from pathlib import Path
from datetime import datetime

from dataset.data_factory import data_provider, get_data_info
from utils.tools import EarlyStopping, adjust_learning_rate, visual_comprehensive, save_results, get_device
from utils.metrics import metric, print_metrics


class ContinualTrainer:
    """æŒç»­å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self, args):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            args: å‚æ•°é…ç½®å¯¹è±¡
        """
        self.args = args
        self.device = get_device(args)
        
        # è®¾ç½®å®éªŒæ ‡è¯†
        self.setting = f"{args.model_id}_{args.data}_sl{args.seq_len}_pl{args.pred_len}_{args.des}"
        
        # åˆ›å»ºä¿å­˜è·¯å¾„
        self.path = os.path.join(args.checkpoints, self.setting)
        Path(self.path).mkdir(parents=True, exist_ok=True)
        
        # æ„å»ºæ¨¡å‹
        self.model = self._build_model()
        
        # æŸå¤±å‡½æ•°
        self.criterion = self._get_criterion()
        
        # ä¼˜åŒ–å™¨
        self.optimizer = None
    
    def _build_model(self):
        """æ„å»ºæ¨¡å‹"""
        print("\n" + "="*70)
        print("æ„å»ºæŒç»­å­¦ä¹ é¢„æµ‹æ¨¡å‹...")
        print("="*70)
        
        # è‡ªåŠ¨è·å–æ•°æ®ç»´åº¦
        input_dim, output_dim = get_data_info(self.args)
        
        # æ›´æ–°argsä¸­çš„ç»´åº¦ä¿¡æ¯
        self.args.input_dim = input_dim
        self.args.output_dim = output_dim
        
        print(f"\næ•°æ®é…ç½®:")
        print(f"  è¾“å…¥ç»´åº¦: {input_dim}")
        print(f"  è¾“å‡ºç»´åº¦: {output_dim}")
        print(f"  åºåˆ—é•¿åº¦: {self.args.seq_len}")
        print(f"  é¢„æµ‹é•¿åº¦: {self.args.pred_len}")
        
        # ä½¿ç”¨æ–°çš„æ¡†æ¶æ„å»ºæ¨¡å‹
        from models.framework import build_continual_forecaster
        
        model = build_continual_forecaster(
            backbone_type=self.args.backbone_type,
            memory_type=self.args.memory_type,
            input_dim=input_dim,
            output_dim=output_dim,
            pred_len=self.args.pred_len,
            seq_len=self.args.seq_len,
            backbone_dim=self.args.d_model,
            backbone_depth=self.args.e_layers,
            backbone_heads=self.args.n_heads,
            neural_memory_batch_size=self.args.neural_memory_batch_size,
            memory_chunk_size=self.args.memory_chunk_size,
            memory_model_type=self.args.memory_model_type,
            fusion_type=self.args.fusion_type,
        ).to(self.device)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        model_info = model.get_model_info()
        print(f"\næ¨¡å‹æ¶æ„:")
        print(f"  Backbone: {model_info['backbone']}")
        print(f"  Memory Unit: {model_info['memory_unit']}")
        print(f"  ç‰¹å¾ç»´åº¦: {model_info['feature_dim']}")
        print(f"  èåˆæ–¹å¼: {model_info['fusion_type']}")
        print(f"  æ€»å‚æ•°é‡: {model_info['total_params']:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°: {model_info['trainable_params']:,}")
        
        if 'memory_config' in model_info:
            print(f"\nè®°å¿†å•å…ƒé…ç½®:")
            for key, value in model_info['memory_config'].items():
                print(f"  {key}: {value}")
        
        print("="*70)
        
        return model
    
    def _get_criterion(self):
        """è·å–æŸå¤±å‡½æ•°"""
        if self.args.loss == 'mse':
            return nn.MSELoss()
        elif self.args.loss == 'mae':
            return nn.L1Loss()
        elif self.args.loss == 'huber':
            return nn.SmoothL1Loss()
        else:
            return nn.MSELoss()
    
    def _get_optimizer(self, params=None):
        """è·å–ä¼˜åŒ–å™¨"""
        if params is None:
            params = self.model.parameters()
        
        if self.args.optimizer == 'adam':
            optimizer = optim.Adam(
                params, 
                lr=self.args.learning_rate,
                weight_decay=self.args.weight_decay
            )
        elif self.args.optimizer == 'adamw':
            optimizer = optim.AdamW(
                params,
                lr=self.args.learning_rate,
                weight_decay=self.args.weight_decay
            )
        elif self.args.optimizer == 'sgd':
            optimizer = optim.SGD(
                params,
                lr=self.args.learning_rate,
                momentum=0.9,
                weight_decay=self.args.weight_decay
            )
        else:
            optimizer = optim.Adam(params, lr=self.args.learning_rate)
        
        return optimizer
    
    def _process_one_batch(self, batch_x, batch_y, cache=None):
        """
        å¤„ç†ä¸€ä¸ªbatch
        
        Args:
            batch_x: è¾“å…¥æ•°æ® [batch_size, seq_len, input_dim]
            batch_y: ç›®æ ‡æ•°æ® [batch_size, label_len + pred_len, output_dim]
            cache: è®°å¿†çŠ¶æ€cache
        
        Returns:
            outputs: é¢„æµ‹ç»“æœ [batch_size, pred_len, output_dim]
            batch_y: çœŸå®æ ‡ç­¾ï¼ˆåªå–pred_lenéƒ¨åˆ†ï¼‰
            next_cache: æ›´æ–°åçš„cacheçŠ¶æ€
        """
        batch_x = batch_x.float().to(self.device)
        batch_y = batch_y.float().to(self.device)
        
        # æ ¹æ®æ˜¯å¦ä¼ å…¥cacheå†³å®šæ˜¯å¦ç»´æŠ¤è®°å¿†çŠ¶æ€
        if cache is not None:
            outputs, next_cache = self.model(batch_x, cache=cache, return_cache=True)
        else:
            outputs, _ = self.model(batch_x, cache=None, return_cache=False)
            next_cache = None
        
        # æå–é¢„æµ‹éƒ¨åˆ†çš„æ ‡ç­¾
        if self.args.label_len > 0:
            batch_y = batch_y[:, -self.args.pred_len:, :]
        
        return outputs, batch_y, next_cache
    
    def train(self):
        """é¢„è®­ç»ƒé˜¶æ®µï¼šåŒæ—¶è®­ç»ƒPå’ŒM"""
        print("\n" + "="*70)
        print(f"å¼€å§‹é¢„è®­ç»ƒï¼ˆåŒæ—¶è®­ç»ƒBackboneå’ŒMemory Unitï¼‰")
        print(f"å®éªŒè®¾ç½®: {self.setting}")
        print("="*70)
        
        # è·å–æ•°æ®
        train_data, train_loader = data_provider(self.args, flag='train')
        vali_data, vali_loader = data_provider(self.args, flag='val')
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.optimizer = self._get_optimizer()
        
        # æ—©åœ
        early_stopping = EarlyStopping(patience=self.args.patience, verbose=True)
        
        # è®­ç»ƒè®°å½•
        train_steps = len(train_loader)
        
        print(f"\nè®­ç»ƒé…ç½®:")
        print(f"  Epochs: {self.args.train_epochs}")
        print(f"  Batch Size: {self.args.batch_size}")
        print(f"  Learning Rate: {self.args.learning_rate}")
        print(f"  Optimizer: {self.args.optimizer}")
        print(f"  Loss: {self.args.loss}")
        print(f"  Steps per Epoch: {train_steps}")
        
        start_time = time.time()
        
        # è®°å½•è®­ç»ƒå†å²
        train_loss_history = []
        val_loss_history = []
        
        for epoch in range(self.args.train_epochs):
            epoch_time = time.time()
            
            # è®­ç»ƒä¸€ä¸ªepoch
            train_loss = self._train_epoch(train_loader, epoch)
            
            # éªŒè¯
            vali_loss = self._validate(vali_loader)
            
            # è®°å½•losså†å²
            train_loss_history.append(train_loss)
            val_loss_history.append(vali_loss)
            
            # æ‰“å°ä¿¡æ¯
            epoch_duration = time.time() - epoch_time
            print(f"\nEpoch {epoch + 1}/{self.args.train_epochs} | "
                  f"Time: {epoch_duration:.2f}s | "
                  f"Train Loss: {train_loss:.6f} | "
                  f"Val Loss: {vali_loss:.6f}")
            
            # æ—©åœæ£€æŸ¥
            early_stopping(vali_loss, self.model, os.path.join(self.path, 'checkpoint.pth'))
            if early_stopping.early_stop:
                print(f"\næ—©åœè§¦å‘ï¼åœ¨ç¬¬ {epoch + 1} è½®åœæ­¢è®­ç»ƒã€‚")
                break
            
            # å­¦ä¹ ç‡è°ƒæ•´
            if epoch > 0:
                adjust_learning_rate(self.optimizer, epoch + 1, self.args, printout=False)
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        best_model_path = os.path.join(self.path, 'checkpoint.pth')
        self.model.load_state_dict(torch.load(best_model_path))
        
        total_time = time.time() - start_time
        print(f"\né¢„è®­ç»ƒå®Œæˆï¼æ€»ç”¨æ—¶: {total_time:.2f}s")
        
        return self.model
    
    def _train_epoch(self, train_loader, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        train_loss = []
        
        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):
            # å‰å‘ä¼ æ’­ï¼ˆè®­ç»ƒæ—¶ä¸ç»´æŠ¤cacheï¼Œæ¯ä¸ªbatchç‹¬ç«‹ï¼‰
            outputs, batch_y, _ = self._process_one_batch(batch_x, batch_y)
            
            # è®¡ç®—æŸå¤±
            loss = self.criterion(outputs, batch_y)
            train_loss.append(loss.item())
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            if self.args.clip_grad > 0:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.clip_grad)
            
            # æ›´æ–°å‚æ•°
            self.optimizer.step()
            
            # æ‰“å°æ—¥å¿—
            if (i + 1) % self.args.log_interval == 0:
                print(f"  Epoch [{epoch + 1}] Step [{i + 1}/{len(train_loader)}] | Loss: {loss.item():.6f}")
        
        return np.mean(train_loss)
    
    def _validate(self, vali_loader):
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        vali_loss = []
        
        with torch.no_grad():
            for batch_x, batch_y, batch_x_mark, batch_y_mark in vali_loader:
                outputs, batch_y, _ = self._process_one_batch(batch_x, batch_y)
                loss = self.criterion(outputs, batch_y)
                vali_loss.append(loss.item())
        
        return np.mean(vali_loss)
    
    def online_test(self, freeze_backbone=False, load_checkpoint=True):
        """
        åœ¨çº¿æµ‹è¯•é˜¶æ®µ
        
        Args:
            freeze_backbone: True=æ¨¡å¼A(ä»…Må­¦ä¹ ), False=æ¨¡å¼B(Må’ŒPéƒ½å­¦ä¹ )
            load_checkpoint: æ˜¯å¦åŠ è½½é¢„è®­ç»ƒçš„checkpoint
        """
        print("\n" + "="*70)
        mode_name = "æ¨¡å¼A: ä»…è®°å¿†å•å…ƒå­¦ä¹ " if freeze_backbone else "æ¨¡å¼B: å…¨æ¨¡å‹å­¦ä¹ "
        print(f"å¼€å§‹åœ¨çº¿æµ‹è¯•: {mode_name}")
        print(f"å®éªŒè®¾ç½®: {self.setting}")
        print("="*70)
        
        # åŠ è½½checkpoint
        if load_checkpoint:
            checkpoint_path = os.path.join(self.path, 'checkpoint.pth')
            if os.path.exists(checkpoint_path):
                self.model.load_state_dict(torch.load(checkpoint_path))
                print(f"âœ“ å·²åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {checkpoint_path}")
            else:
                print(f"âš  æœªæ‰¾åˆ°checkpoint: {checkpoint_path}")
        
        # è·å–æµ‹è¯•æ•°æ®
        test_data, test_loader = data_provider(self.args, flag='test')
        
        # æ ¹æ®æ¨¡å¼é…ç½®æ¨¡å‹å’Œä¼˜åŒ–å™¨
        if freeze_backbone:
            # æ¨¡å¼A: å†»ç»“Backboneï¼Œåªè®©Memory Unitå­¦ä¹ 
            print("\né…ç½®æ¨¡å¼A:")
            print("  - å†»ç»“Backboneå‚æ•°")
            print("  - Memory Unité€šè¿‡å†…ç½®æœºåˆ¶è‡ªåŠ¨æ›´æ–°")
            print("  - cacheè·¨batchä¼ é€’ï¼Œå®ç°æŒç»­å­¦ä¹ ")
            
            self.model.eval()  # evalæ¨¡å¼ï¼ˆä½†ä¸å½±å“NeuralMemoryçš„å†…éƒ¨æ›´æ–°ï¼‰
            
            # å†»ç»“Backbone
            for param in self.model.backbone.parameters():
                param.requires_grad = False
            
            optimizer = None  # ä¸éœ€è¦å¤–éƒ¨optimizer
            
        else:
            # æ¨¡å¼B: På’ŒMéƒ½å­¦ä¹ 
            print("\né…ç½®æ¨¡å¼B:")
            print(f"  - Backboneå’ŒMemory Unitéƒ½å‚ä¸å­¦ä¹ ")
            print(f"  - ä½¿ç”¨åœ¨çº¿å­¦ä¹ ç‡: {self.args.online_lr}")
            print("  - cacheè·¨batchä¼ é€’ï¼Œå®ç°æŒç»­å­¦ä¹ ")
            
            self.model.train()
            
            # è§£å†»æ‰€æœ‰å‚æ•°
            for param in self.model.parameters():
                param.requires_grad = True
            
            optimizer = optim.Adam(
                filter(lambda p: p.requires_grad, self.model.parameters()),
                lr=self.args.online_lr
            )
        
        print("="*70 + "\n")
        
        # åœ¨çº¿æµ‹è¯•å¾ªç¯
        preds = []
        trues = []
        losses = []
        
        # ğŸ”‘ å…³é”®ï¼šåˆå§‹åŒ–cacheä»¥ç»´æŠ¤è®°å¿†çŠ¶æ€
        cache = None
        
        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):
            # ğŸ”‘ å…³é”®ï¼šä¼ å…¥cacheç»´æŒè®°å¿†çŠ¶æ€
            outputs, batch_y, cache = self._process_one_batch(batch_x, batch_y, cache)
            
            # è®¡ç®—æŸå¤±
            loss = self.criterion(outputs, batch_y)
            loss_per_sample = F.mse_loss(outputs, batch_y, reduction='none').mean(dim=(1,2))
            losses.extend(loss_per_sample.detach().cpu().numpy().tolist())
            
            # åœ¨çº¿æ›´æ–°ï¼ˆä»…æ¨¡å¼Béœ€è¦ï¼‰
            if optimizer is not None:
                optimizer.zero_grad()
                loss.backward()
                
                if self.args.clip_grad > 0:
                    torch.nn.utils.clip_grad_norm_(
                        filter(lambda p: p.requires_grad, self.model.parameters()), 
                        self.args.clip_grad
                    )
                
                optimizer.step()
            
            # æ”¶é›†é¢„æµ‹å’ŒçœŸå®å€¼
            pred = outputs.detach().cpu().numpy()
            true = batch_y.detach().cpu().numpy()
            
            preds.append(pred)
            trues.append(true)
            
            # æ‰“å°è¿›åº¦
            if (i + 1) % 10 == 0:
                recent_loss = np.mean(losses[-320:]) if len(losses) >= 320 else np.mean(losses)
                print(f"  è¿›åº¦: Batch [{i+1}/{len(test_loader)}] | Recent Loss: {recent_loss:.6f}")
        
        return self._finalize_test_results(preds, trues, losses, test_data, freeze_backbone)
    
    def _finalize_test_results(self, preds, trues, losses, test_data, freeze_backbone):
        """æ•´ç†æµ‹è¯•ç»“æœå¹¶è¿›è¡Œè¯„ä¼°ã€å¯è§†åŒ–"""
        
        # åˆå¹¶ç»“æœ
        preds = np.concatenate(preds, axis=0)
        trues = np.concatenate(trues, axis=0)
        
        print(f"\né¢„æµ‹å½¢çŠ¶: {preds.shape}")
        print(f"çœŸå®å€¼å½¢çŠ¶: {trues.shape}")
        
        # åæ ‡å‡†åŒ–
        if hasattr(test_data, 'inverse_transform'):
            preds_orig = test_data.inverse_transform(preds.reshape(-1, preds.shape[-1]))
            trues_orig = test_data.inverse_transform(trues.reshape(-1, trues.shape[-1]))
            preds_orig = preds_orig.reshape(preds.shape)
            trues_orig = trues_orig.reshape(trues.shape)
        else:
            preds_orig = preds
            trues_orig = trues
        
        # è®¡ç®—æŒ‡æ ‡
        mae, mse, rmse, mape, mspe, rse, corr = metric(preds_orig, trues_orig)
        print_metrics(mae, mse, rmse, mape, mspe, rse, corr)
        
        # ä¿å­˜ç»“æœ
        if self.args.save_pred:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            mode_suffix = 'memory_only' if freeze_backbone else 'full_model'
            result_path = os.path.join(self.args.result_path, f'{self.setting}_online_{mode_suffix}_{timestamp}.csv')
            save_results(trues_orig, preds_orig, losses, result_path)
            print(f"âœ“ ç»“æœå·²ä¿å­˜åˆ°: {result_path}")
        
        # å¯è§†åŒ–
        if self.args.save_fig:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            mode_suffix = 'memory_only' if freeze_backbone else 'full_model'
            fig_path = os.path.join(self.args.fig_path, f'{self.setting}_online_{mode_suffix}_{timestamp}.jpg')
            visual_comprehensive(
                trues_orig.flatten(), 
                preds_orig.flatten(), 
                losses if len(losses) > 0 else None,
                fig_path,
                train_size=None
            )
            print(f"âœ“ å¯è§†åŒ–å·²ä¿å­˜åˆ°: {fig_path}")
        
        return mae, mse, rmse, mape, mspe, rse


if __name__ == '__main__':
    print("æ–°çš„è®­ç»ƒå™¨æ¨¡å—å·²åˆ›å»º")
    print("è¯·é€šè¿‡titans_main.pyè¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹")

